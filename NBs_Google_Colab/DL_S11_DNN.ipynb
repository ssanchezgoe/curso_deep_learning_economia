{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S11_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOG1sqNdKmGisOhTNRdp+VS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/DL_S11_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thBYXFC53-lF"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"140px\" src=\"https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1> Curso Deep Learning: Economía</h1>\n",
        "\n",
        "## S11: Deep Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqEK8WJ7uQyo"
      },
      "source": [
        "# 1. Problema del desvanecimiento/explosión del gradiente.\n",
        "\n",
        "Como hemos visto, las redes neuronales son entrenadas usando el algoritmo del **descenso del gradiente estocástico** (hasta ahora) conjuntamente con el de **propagación hacia atrás** (o retropropagación). \n",
        "\n",
        "Lo anterior consiste, en primer lugar, en el cálculo del error de la predicción realizada por el modelo y el uso de este error para estimar un gradiente que es usado para actualizar cada peso en la red, de tal forma que se reduzca el error en la siguiente iteración. Este error del gradiente es propagado hacia atrás a lo largo de la red, desde la salida a la capa de entrada.\n",
        "\n",
        "Normalmente, se desea entrenar la red neuronal con muchas capas, con la intensión de que el incremento del número de capas aumente la capacidad predictiva de la red, haciéndola capaz de aprender un dataset de gran tamaño. Por otra parte, se espera también que al aumentar el número de capas se logre representar de forma eficiente funciones de mapeo más complejas de los datos de entrada a las predicciones a la salida de la red.\n",
        "\n",
        "Un problema en el entrenamiento de redes con muchas capas, **deep neural networks**, es que el gradiente se **desvanece o explota** dramáticamente a medida que se propaga hacia atrás a lo largo de la red. En el caso del desvanecimiento del gradiente, la actualización del error alcanzado en las capas cercanas a la entrada puede ser tan pequeño que tenga un efecto despreciable. El desvanecimiento del gradiente dificulta saber en qué dirección se deben variar los parámetros para obtener una mejora en la función de coste.\n",
        "\n",
        "El término del **desvanecimiento/explosión** del gradiente se refiera al hecho de que en un red neuronal alimentada hacia adelante el error retropropagado normalmente decrece (o incrementa) en función de la distancia desde la capa final.\n",
        "\n",
        "Además del problema de desvanecimiento, el error del gradiente puede ser inestable en redes neuronales profundas, sufriendo cambios abruptos o explosiones (divergencias), en donde el gradiente crece exponencialmente a medida que se propaga hacia atrás. Este problema se conce como problema del gradiente explosivo:\n",
        "\n",
        "El problema de los gradientes que se desvanecen es un problema particular en las redes neuronales recurrentes, ya que la actualización de la red implica “desenrrollar” la red, creando una red muy profunda que requiere la actualización de los pesos. Una red neuronal recurrente modesta puede tener entre 200 a 400 pasos de tiempo de entrada, lo que resulta conceptualmente en una red muy profunda.\n",
        "\n",
        "El problema de los gradientes que se desvanecen se puede manifestar en un perceptrón multicapas mediante una rata baja de la mejora del modelo durante el entrenamiento, y, quizas, en una convergencia temprana, es decir, el entrenamiento continuo no implica una mejora de la precisión del modelo. La inspección de los cambios de los pesos durante el entrenamiento, nos debería llevar a mayores cambios (que implica un mayor aprendizaje) en las capas cercanas a la salida  y cambios menores en las capas para cada paso de tiempo de entrada. \n",
        "\n",
        "Existen varias técnicas que pueden ser usadas para reducir el impacto de los gradientes que se desvanecen en el caso de redes neuronales alimentadas hacia adelante, los más notables de ellos son :\n",
        "\n",
        "-\tLos esquema de inicialización de pesos alternantes.\n",
        "-\tEl uso de funciones de activación alternativas.\n",
        "\n",
        "Se han estudiado diferentes aproximaciones para el entrenamiento de redes neuronales profundas (tanto para redes alimentadas hacia adelante como las recurrentes), en un esfuerzo para abordar los gradientes que se desvanecen, como el pre-entrenamiento, una mejora en el escalado aleatorio inicial, las mejoras en los métodos de optimización, el estudio de arquitecturas específicas, las inicializaciones ortogonales, etc.\n",
        "\n",
        "En esta sección echaremos una mirada más cerca al uso de funciones de activación y esquemás alternativos de inicialización de los pesos, para poder entrenar redes neuronales más profundas. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsK5hTp1v65_"
      },
      "source": [
        "## Problema inducido por la funciones de activación:\n",
        "\n",
        "Como vimos en la clase anterior, el algoritmo de retropropagación requiere de las derivadas de las funciones de activación, lo que implica que algunas de ellas favorecen el desvanecimento del gradiente.\n",
        "\n",
        "Veamos que ocurre con las derivadas de las funciones de activación usadas en las capas ocultas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRTUflP1Fq0"
      },
      "source": [
        "### Función sigmoide:\n",
        "\n",
        "La función sigmoide se define de la forma \n",
        "\n",
        "$$S(x)=\\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "en donde la gráfica tiene la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSEqK2-vykzQ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "x = np.linspace(-10,10,100)\n",
        "\n",
        "plt.plot(x,sigmoid(x),'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLfT6Zi7zvXZ"
      },
      "source": [
        "La cual tiene un rango definido en el intervalo abierto $(0,1)$.\n",
        "\n",
        "La derivada de esta función, podemos calcularla como:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5UR7WrxRSA"
      },
      "source": [
        "from sympy import *\n",
        "\n",
        "x = Symbol('x')\n",
        "f = 1/(1+exp(-x))\n",
        "f.diff(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxFGEXXez2CU"
      },
      "source": [
        "def sigmoid_deff(x):\n",
        "  return np.exp(-x)/(1 + np.exp(-x))**2\n",
        "\n",
        "x = np.linspace(-10,10,100)\n",
        "plt.plot(x,sigmoid_deff(x),'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GukXp4b72I8V"
      },
      "source": [
        "Veamos además, que el mapero de la función de activación **sigmoide** genera una concentración en las colas de la función, en donde la derivada se hace cero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WehM-fi92IJX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "x=np.linspace(-5,5,100)\n",
        "\n",
        "y_sigmoid = keras.activations.sigmoid(x)\n",
        "\n",
        "y_sigmoid_diff = np.gradient(y_sigmoid)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(x,y_sigmoid,'.')\n",
        "plt.title('Función de Activación sigmoide')\n",
        "plt.subplot(122)\n",
        "plt.plot(x,y_sigmoid_diff,'.')\n",
        "plt.title('Derivada de la función de Activación sigmoide')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkTylKl50duE"
      },
      "source": [
        "**Problemas de la función sigmoide:**\n",
        "\n",
        "**1) Desvanecimiento del gradiente:** Vemos que la derivada de esta función está muy cerca a cero en todas partes, excepto en el ancho representado por la gaussian. Lo anterior representa el caso a favor del desvanecimiento del gradiente. Por este motivo, la función tangente hiperbólica fue usada en su lugar.\n",
        "\n",
        "**2) Función no centrada en cero:** La función sigmoide representa una función no centrada en cero, generando valores positivos después de que la activación tenga lugar. Sin entrar en detalles, este hecho ocaciona que el gradiente de los pesos los convierta a todos en positivos o negativos, lo que dificulta la actualización del gradiente, comportándose de forma erratica. A este problema se le conoce como el [problema de funciones no centradas en cero](https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6Afe9W01Ssw"
      },
      "source": [
        "#### Función de activación Tanh:\n",
        "\n",
        "La función de activación tangente hiperbólica se define como:\n",
        "\n",
        "$$\\text{Tanh}(x)=\\frac{e^x-​e^{​‑x}}{e^x+​e^{‑x}}$$\n",
        "\n",
        "cuya gráfica es:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrX9LZW_0cvZ"
      },
      "source": [
        "x = np.linspace(-7,7,100)\n",
        "\n",
        "plt.plot(x,np.tanh(x),'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haqzjfjQxJ6j"
      },
      "source": [
        "Cuyo rango es el intervalo abierto $(-1,1)$.\n",
        "\n",
        "La derivada de esta función es:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA6FiABU3sL0"
      },
      "source": [
        "x = Symbol('x')\n",
        "f = (exp(x)-exp(-x))/(exp(x)+exp(-x))\n",
        "f.diff(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIVrXSsX4NXv"
      },
      "source": [
        "def tanh_diff(x):\n",
        "  return (-np.exp(x)+np.exp(-x))*(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))**2 + 1\n",
        "\n",
        "x = np.linspace(-7,7,100)\n",
        "\n",
        "plt.plot(x,tanh_diff(x),'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lByzDFrq4zA7"
      },
      "source": [
        "Veamos además, que el mapeo de la función de activación la función **tanh** genera una concentración de puntos en las colas de la función, en donde su derivada se hace cero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyPjYse5tV17"
      },
      "source": [
        "# función tanh\n",
        "x=np.linspace(-5,5,100)\n",
        "\n",
        "y_tanh = keras.activations.tanh(x)\n",
        "\n",
        "#Derivada\n",
        "y_tanh_diff = np.gradient(y_tanh)\n",
        "\n",
        "# Gráfica de la función de activación y su derivada:\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(x,y_tanh,'.')\n",
        "plt.title('Función de Activación Tanh')\n",
        "plt.subplot(122)\n",
        "plt.plot(x,y_tanh_diff,'.')\n",
        "plt.title('Derivada de la función de Activación Tanh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REFqXqcm1IQ4"
      },
      "source": [
        "**Problemas de la función de activación tanh**\n",
        "\n",
        "**1) Desvanecimiento del gradiente:** A pesar de se ha demostrado que el desempeño de la función de activación es mejor, dado su rango $(-1,1)$, la derivada de esta función es muy cercana a cero en todo el dominio, excepto dentro de un intervalo pequeño centrado en el origen y determinado por el ancho de la gaussiana. Problema similar al de la función sigmoide.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1c5j09O6Nnq"
      },
      "source": [
        "### Función de activación ReLU:\n",
        "\n",
        "Con el fin de corregir el problema introduccido por el desvanecimiento del gradiente al usar las funciones de activación **sigmoide** y **tangente hiperbólica**, se introduce la función **ReLU**.\n",
        "\n",
        "Vemos las características de esta función y su derivada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsPQUOig6Ove"
      },
      "source": [
        "x=np.linspace(-50,50,101)\n",
        "\n",
        "# Función de activación ReLU\n",
        "y_relu = keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)\n",
        "\n",
        "# Derivada de la función de activación ReLU\n",
        "y_relu_diff = np.gradient(y_relu)\n",
        "\n",
        "# Gráfica de la función de activación ReLU y su derivada:\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(x,y_relu,'.')\n",
        "plt.title('Función de Activación ReLU')\n",
        "plt.subplot(122)\n",
        "plt.plot(x,y_relu_diff,'.')\n",
        "plt.title('Derivada de la función de Activación ReLU')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIrXQAEM4HLS"
      },
      "source": [
        "**Problemas de la función de activación ReLU**\n",
        "\n",
        "**1) Dying ReLU:**  El problema principal de la función de activación ReLU está asociaado con el hecho de asignar un valor cero a todos los valores negativos provenientes del ajuste lineal. Decimos que una neurona se encuentra muerta cuando el ajuste arroja valores negativos. Como vemos en la gráfica de la derivada ReLU, ésta es cero en los valores negativos de la función. Una vez que la neurona se vuelve \"negativa\", es muy poco probable que se recupere, y se convertirá en una neurona muerta. El problema de las neuronas muertas es que corresponenden a unidades inservibles, incapaces de contribuir en la clasificación. A este problema se le conoce con el nombre en inglés de [Dying ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7). Una solución a este problema es aplicar una función de activación conocida como LeakyReLU, la cual puede ser consultada en la referencia anterior y no abordaremos en este módulo.\n",
        "\n",
        "A pesar de este problema, veremos que la función de activación resuelve el problema del desvanecimiento del gradiente en redes neuronales profundas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q762AKoyUvXI"
      },
      "source": [
        "### Función de activación SeLU:\n",
        "\n",
        "A continuación, hablaremos brevemente de la funciones SELU (Scaled Exponencial Linear Units), Las cuales aparecen por primera vez en el siguiente\n",
        "[artículo](https://arxiv.org/pdf/1706.02515.pdf).\n",
        "\n",
        "Para una explicación detallada de la función de activación SELU, pueden consultar la siguiente \n",
        "[referencia](https://towardsdatascience.com/gentle-introduction-to-selus-b19943068cd9).\n",
        "\n",
        "La forma de la función SELU y su derivada es la siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWxHBqGqB0h7"
      },
      "source": [
        "x=np.linspace(-20,20,100)\n",
        "\n",
        "y_selu = keras.activations.selu(x)\n",
        "\n",
        "y_selu_diff = np.gradient(y_selu)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(x,y_selu,'.')\n",
        "plt.title('Función de Activación ReLU')\n",
        "plt.subplot(122)\n",
        "plt.plot(x,y_selu_diff,'.')\n",
        "plt.title('Derivada de la función de Activación ReLU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2dQ2G22fGST"
      },
      "source": [
        "De esta  gráfica podemos extraer dos características principales de la función de activación SELU:\n",
        "\n",
        "1. El lado derecho de la función, es similar a la función de activación ReLU.\n",
        "\n",
        "2. El lado izquierdo, parace aproximarse a un gradiente zero, que es justo lo que queriamos evitar. No obstante la función SELU realiza un proceso aparte del control del gradiente conocído como **normalización**.\n",
        "\n",
        "Respecto a la normalización podemos tener tres casos:\n",
        "\n",
        "1. Que la normalización tenga lugar a la entrada de la red, por ejemplo cuando se reescala un intervalo de valor de píxeles que va de 0-255 a uno entre 0-1. Este tipo de normalización es una buena practica en ML.\n",
        "\n",
        "2. En el caso de  NN, existe un normalización relevante conocida como **batch normalization**, la cual tiene lugar entre las capas de la red, transformandose las salidas de tal manera que el valor medio es cero y la desviación estándar sea uno. La principal ventaja de este procedimiento es que limita los valores y hace menos probable que ocurran valores extremales donde se hace cero la derivada. \n",
        "\n",
        "3. También en el caso de NN, la normalización puede tener lugar **internamente**, como en el caso de la función SELU. La idea principal es que cada capa preserva el valor medio y la varianza de la capa anterior, evitándose de esta forma valores extremales. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaY5J6zsfGor"
      },
      "source": [
        "**Problemas de la función SELU**\n",
        "\n",
        "1) Función reciente que necesita ser estudiada.\n",
        "\n",
        "2) Computacionalmente costosa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HI3HjvkFbi"
      },
      "source": [
        "# Redes Neuronales Profundas\n",
        "\n",
        "En las clases previas introdujimos las redes neuronales artificiales y vimos algunos ejemplos de cómo entrenarlas. Pero estas eran redes muy poco profundas, con solo unas pocas capas ocultas. ¿Qué sucede si necesita abordar un problema muy complejo, como detectar cientos de tipos de objetos en imágenes de alta resolución? (Como ejemplo, considere el DIUx xView 2018 Detection Challenge http://xviewdataset.org/). Para estos casos, es posible que se necesite entrenar un DNN mucho más profunda, tal vez con 10 capas o más, cada una con cientos de neuronas, conectadas por cientos de miles de conexiones. Aparecerán algunos problemas cuando intentes entrenar redes neuronales realmente profundas, algunos de ellos son:\n",
        "\n",
        "* Vanishing gradients y exploding gradients que afecta las redes neuronales profundas y hace que las capas inferiores sean muy difíciles de entrenar.\n",
        "\n",
        "* Es posible que no tengas suficientes datos de entrenamiento para una red tan grande, o puede ser demasiado costoso etiquetarlos.\n",
        "\n",
        "* El entrenamiento puede ser extremadamente lento.\n",
        "\n",
        "* un modelo con millones de parámetros correría el riesgo de generar overfitting, especialmente si no hay suficientes instancias de entrenamiento o si son demasiado ruidosas.\n",
        "\n",
        "En ésta lección veremos algunas técnicas para resolver algunos de estos problemas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTMqm7Ic3IJ0"
      },
      "source": [
        "## Ejemplificación del desvanecimiento del gradiente:\n",
        "\n",
        "A continuación haremos uso de un ejemplo de juguete para ilustrar el problema del desvanecimiento del gradiente dos gupos de puntos (clase 1 y clase 0) con una distribución circular.\n",
        "\n",
        "Recordemos como crear un dataset a partir de la clase `make_circles` del módulo `datasets` de la librería `sklean`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuZHyCkIlt6m"
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generacíon de dos grupos de círculos.\n",
        "\n",
        "X, y= make_circles(n_samples = 1000, noise=0.1, random_state=1)\n",
        "\n",
        "# selección de las clases para graficarlas \n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], label=str(0),c='k') # extacción puntos según clase (0)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], label=str(1),c='g') # extacción puntos según clase (1)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ldMIh7Gc3MT"
      },
      "source": [
        "**Explicación del código:**\n",
        "\n",
        "En la primeta parte buscamos todos indices con etiquetas`y==0` y etiquetas `y==1`. Para eso usamos la función de `numpy` `where`. Para el caso en que `y == 0`, tenemos que:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck56IZNrmfgZ"
      },
      "source": [
        "np.where(y == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi6i2XwJdjR2"
      },
      "source": [
        "Si este array (tupla en realidad), se la pasamos en la indexación `X`, obtenemos las instancias (puntos `(X[idx,0],X[idx,1])`), tenemos los valores de las instancias para cada clase (0 y 1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F95GaXGAms33"
      },
      "source": [
        "X[np.where(y==0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6UjroTXb38C"
      },
      "source": [
        "# Preprocesado de los datos:\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scale input data to [-1,1]\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXOGwaBsnC0r"
      },
      "source": [
        "# División de los datos en entrenamiento y evaluación\n",
        "n_train = 500\n",
        "train_X, test_X = X[:n_train, :], X[n_train:, :]\n",
        "train_y, test_y = y[:n_train], y[n_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOttMxf4nR-7"
      },
      "source": [
        "### Modelo del perceptrón multicapas para el problema de dos círculos:\n",
        "\n",
        "Empecemos por ilustrar el caso en que temos un ajuste adecuado del modelo de de una red neuronal poco profunda consituida por una capa de entrada, con una función de activación `tanh` y una capa de salida `sigmoid`. Este modelo, no debe sufrir de problemas de desvanecimiento/explosión del gradiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6024eAMvnOgx"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Definición del modelo poco profundo\n",
        "modelA= keras.models.Sequential()\n",
        "init = keras.initializers.RandomUniform(minval=0, maxval=1)\n",
        "modelA.add(keras.layers.Dense(5, input_dim=2, activation='tanh', kernel_initializer=init))\n",
        "modelA.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer=init))\n",
        "\n",
        "# Compilación del modelo\n",
        "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
        "modelA.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
        "modelA.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glcrH8e2jTwc"
      },
      "source": [
        "\n",
        "# Ajuste del modelo:\n",
        "history = modelA.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3RaPEsfjtIL"
      },
      "source": [
        "\n",
        "# evaluate the model\n",
        "_, train_acc = modelA.evaluate(train_X, train_y, verbose=0)\n",
        "_, test_acc = modelA.evaluate(test_X, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "# plot training history\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5By1rHXkOdB"
      },
      "source": [
        "y_pred=modelA.predict_classes(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AENztF22kaYL"
      },
      "source": [
        "\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], label=str(0)) # extacción puntos según clase (0)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], label=str(1)) # extacción puntos según clase (1)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncNbcbkXkgOV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejMfG_vpkbsz"
      },
      "source": [
        "y_pred=y_pred.reshape((-1))\n",
        "plt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], label=str(0)) # extacción puntos según clase (0)\n",
        "plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], label=str(1)) # extacción puntos según clase (1)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqYcpJz6pVWw"
      },
      "source": [
        "### Creación de un modelo más profundo con fución de activación tanh:\n",
        "\n",
        "Veamos ahora cómo un modelo de una red neuronal más profunda, genera un modelo menos predictivo devido al problema del desvanecimiento del gradiente. La red consta de una capa de entrada con un a función de activación `tanh`, y 4 capas ocultas, también definidas con una función de activación `tanh`, y una capa de salida con una función de activación `sigmoid`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cokhh_4SrTB0"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# define model\n",
        "init = keras.initializers.RandomUniform(minval=0, maxval=1)\n",
        "modelB = keras.models.Sequential()\n",
        "modelB.add(keras.layers.Dense(5, input_dim=2, activation='tanh', kernel_initializer=init))\n",
        "modelB.add(keras.layers.Dense(5, activation='tanh', kernel_initializer=init))\n",
        "modelB.add(keras.layers.Dense(5, activation='tanh', kernel_initializer=init))\n",
        "modelB.add(keras.layers.Dense(5, activation='tanh', kernel_initializer=init))\n",
        "modelB.add(keras.layers.Dense(5, activation='tanh', kernel_initializer=init))\n",
        "modelB.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer=init))\n",
        "# compile model\n",
        "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
        "modelB.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "# fit model\n",
        "history = modelB.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=500, verbose=0)\n",
        "# evaluate the model\n",
        "_, train_acc = modelB.evaluate(train_X, train_y, verbose=0)\n",
        "_, test_acc = modelB.evaluate(test_X, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "# plot training history\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXhJin7FlFFQ"
      },
      "source": [
        "y_pred=modelB.predict_classes(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--eoHTNglJXU"
      },
      "source": [
        "y_pred=y_pred.reshape((-1))\n",
        "plt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], label=str(0)) # extacción puntos según clase (0)\n",
        "plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], label=str(1)) # extacción puntos según clase (1)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy5RuEZiv19Z"
      },
      "source": [
        "### Creación de un modelo más profundo con fución de activación ReLU:\n",
        "\n",
        "Ilustremos ahora cómo, con el simple hecho de cambiar las función de activación de la capa de entrada y las capas ocultas del anterior modelo por una función de activación `ReLU`, se mejora el problema del desvanecimiento del gradiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcg6cVAXnWJU"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# define model\n",
        "modelC = keras.models.Sequential()\n",
        "modelC.add(keras.layers.Dense(5, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "modelC.add(keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'))\n",
        "modelC.add(keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'))\n",
        "modelC.add(keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'))\n",
        "modelC.add(keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'))\n",
        "modelC.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
        "modelC.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "# fit model\n",
        "history = modelC.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=500, verbose=0)\n",
        "# evaluate the model\n",
        "_, train_acc = modelC.evaluate(train_X, train_y, verbose=0)\n",
        "_, test_acc = modelC.evaluate(test_X, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "# plot training history\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr6PLW7-lf8Z"
      },
      "source": [
        "y_pred=modelC.predict_classes(X)\n",
        "y_pred=y_pred.reshape((-1))\n",
        "plt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], label=str(0)) # extacción puntos según clase (0)\n",
        "plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], label=str(1)) # extacción puntos según clase (1)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiLyGMKqA8zw"
      },
      "source": [
        "Finalmente, veamos que las funciones de activación `SeLU`, también  ayudan a resolver el problema del desvanecimiento/explosión del gradiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzAjS01isz4-"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# define model\n",
        "modelE = keras.models.Sequential()\n",
        "modelE.add(keras.layers.Dense(5, input_dim=2, activation='selu', kernel_initializer='lecun_normal'))\n",
        "modelE.add(keras.layers.Dense(5, activation='selu', kernel_initializer='lecun_normal'))\n",
        "modelE.add(keras.layers.Dense(5, activation='selu', kernel_initializer='lecun_normal'))\n",
        "modelE.add(keras.layers.Dense(5, activation='selu', kernel_initializer='lecun_normal'))\n",
        "modelE.add(keras.layers.Dense(5, activation='selu', kernel_initializer='lecun_normal'))\n",
        "modelE.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
        "modelE.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "# fit model\n",
        "history = modelE.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=500, verbose=0)\n",
        "# evaluate the model\n",
        "_, train_acc = modelE.evaluate(train_X, train_y, verbose=0)\n",
        "_, test_acc = modelE.evaluate(test_X, test_y, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "# plot training history\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GvPrzNurTcF"
      },
      "source": [
        "# Recapitulación del problema del desvanecimiento/explosión del gradiente.\n",
        "\n",
        "Cuando se realiza la propagación hacia atrás, los gradientes a menudo se hacen cada vez más pequeños a medida que el algoritmo avanza hacia las capas inferiores. Como resultado, el algoritmo de Descenso de degradado deja los pesos de conexión de la capa inferior prácticamente sin cambios, y el entrenamiento nunca converge en una buena solución. Esto es lo que se conoce como Vanishing Gradients Problems. En algunos otros casos, puede suceder lo contrario: los gradientes pueden crecer más y más, por lo que muchas capas obtienen actualizaciones de peso increíblemente grandes y el algoritmo diverge. Esto es lo que se conoce como Exploding Gradients Problems.  \n",
        "\n",
        "\n",
        "Aunque este comportamiento se ha observado empíricamente durante bastante tiempo (fue una de las razones por las que las redes neuronales profundas fueron abandonadas durante mucho tiempo), apenas alrededor de 2010 se logró un progreso significativo en su comprensión. Xavier Glorot y Yoshua Bengio descubrieron que el problema estaba en la combinación de la popular función de activación sigmoid (y tambien la hyperbolica aunque en menor medida) y la técnica de inicialización de pesos que era más popular en ese momento (la inicialización aleatoria usando una distribución normal con una media de 0 y un estándar desviación de 1).\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/Dbt0lIL.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFUhoXCb5DEg"
      },
      "source": [
        "# Inicialización de Glorot and He\n",
        "\n",
        "Glorot y Bengio proponen una forma de aliviar significativamente este problema. Argumentan que necesitamos la varianza de la\n",
        "Las salidas de cada capa deben ser iguales a la varianza de sus entradas, y también necesitamos que los gradientes tengan la misma varianza antes y después de fluir a través de una capa en la dirección inversa. En realidad, no es posible garantizar ambos a menos que la capa tenga el mismo número de entradas y neuronas (que en general no es cierto), pero propusieron una buena solución que ha demostrado funcionar muy bien en la práctica: los pesos de cada capa (para el caso de la funcion de activacion sigmoid) deben inicializarse aleatoriamente como (Esta es la inicialización de Glorot):\n",
        "\n",
        "Distribución normal con media 0 y varianza\n",
        "\\begin{equation}\n",
        "\\sigma^2 = \\frac{1}{fan_{avg}}\n",
        "\\end{equation}\n",
        "\n",
        "O una distribución uniforme entre $−r$ and $+r$,\n",
        "\\begin{equation}\n",
        "r = \\sqrt{\\frac{3}{fan_{avg}}} = \\sqrt{3\\sigma^2}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "donde \n",
        "\n",
        "\n",
        "* $fan_{avg} = \\frac{1}{2} (fan_{in} + fan_{out})$\n",
        "\n",
        "\n",
        "* $fan_{in}$ es el numero de neuronas de entrada \n",
        "\n",
        "* $fan_{out}$ es el numero de neuronas de salida \n",
        "\n",
        "\n",
        "La estrategia de inicialización para la función de activación ReLUy sus variantes es (Esta es la inicialización de He) :\n",
        "\n",
        "Distribución normal con media 0 y varianza\n",
        "\\begin{equation}\n",
        "\\sigma^2 = \\frac{2}{fan_{avg}}\n",
        "\\end{equation}\n",
        "\n",
        "O una distribución uniforme entre $−r$ and $+r$,\n",
        "\\begin{equation}\n",
        "r = \\sqrt{\\frac{6}{fan_{avg}}} = \\sqrt{3\\sigma^2}\n",
        "\\end{equation}\n",
        "\n",
        "podemos resumir entonces la forma mas comun de inicialzar los pesos en la sigueinte tabla \n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"200px\" src=\"https://i.imgur.com/hBQ26T3.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd3KJlCiEXZt"
      },
      "source": [
        "Por defecto, Keras usa la inicialización de Glorot con una distribución uniforme, pero esto puede ser cambiado. Veamos un ejemplo de como hacerlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNbYGh0LPm37"
      },
      "source": [
        "importemos algunas librerias que seran de utilidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDmQwTTIhnEI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "from sklearn.datasets import make_circles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzG2fsMiPrLf"
      },
      "source": [
        "Resolveremos el problema de las dos espirales, para esto comencemos con crear las espirales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idSI8-3xhhs9"
      },
      "source": [
        "def twospirals(n_points, noise=0.5):\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * 780 * (2*np.pi)/360\n",
        "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
        "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
        "            np.hstack((np.zeros(n_points),np.ones(n_points))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8grmVwO3lHlM"
      },
      "source": [
        "X, y =  twospirals(1000, noise=0.8)\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap='winter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH0uObEnlZAW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime, os\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVRWCKP_P0yj"
      },
      "source": [
        "Dividamos los datos en un set de entrenamiento y otro de testeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b0-9TKcmWnf"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulDJQhr2P4Yp"
      },
      "source": [
        "escalemos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h2QKX5oq-zY"
      },
      "source": [
        "scale = StandardScaler()\n",
        "X_train = scale.fit_transform(X_train)\n",
        "X_test = scale.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcOUImkq0Rbn"
      },
      "source": [
        "plt.figure(figsize=(7,7))\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='winter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE7PhskHjNHQ"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py_p7N97jPvL"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iisnFo2OP7gz"
      },
      "source": [
        "procedamos a construir nuestro modelo y a ilustrar como incializar los pesos de diferentes formas dependiendo la funcion de activacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br6J_3ypmj9p"
      },
      "source": [
        "model = keras.models.Sequential([                                \n",
        "                                keras.layers.Dense(40, activation='relu', kernel_initializer='he_normal',input_shape= (2,)),\n",
        "                                keras.layers.Dense(12, activation='tanh',kernel_initializer='glorot_normal'),\n",
        "                                keras.layers.Dense(40, activation='relu',kernel_initializer='he_uniform'),\n",
        "                                keras.layers.Dense(12, activation='tanh',kernel_initializer='glorot_normal'),\n",
        "                                keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
        "                                ])\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaoLNNgqoF-H"
      },
      "source": [
        "history = model.fit(X_train,y_train, epochs=200 ,verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9rYqg6goZGK"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md0DV1jnuHuA"
      },
      "source": [
        "model.evaluate(X_test,y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9vVsYz7I8K2"
      },
      "source": [
        "y_fit =  model.predict_classes(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLbFJ-9NIymp"
      },
      "source": [
        "fig , ax = plt.subplots(1,2, figsize=(8,6))\n",
        "ax[0].scatter(X_test[:,0], X_test[:,1], c=y_test, cmap='winter')\n",
        "ax[0].set_title('true_model')\n",
        "ax[1].scatter(X_test[:,0], X_test[:,1], c=y_fit[:,0], cmap='winter')\n",
        "ax[1].set_title('predicted_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBq63u5YOQyy"
      },
      "source": [
        "Por defecto, Keras implementa la inicialización He con una distribución uniforme basada en $fan_{in}$. Si desea la inicialización de He con una distribución uniforme pero basada en $fan_{avg}$, puede usar el inicializador VarianceScaling de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pjyC2p-OlAb"
      },
      "source": [
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n4j-8WTPIJJ"
      },
      "source": [
        "mas informacion de la versatilidad a la hora de inicializar los pesos puede ser encontrado en https://keras.io/initializers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-zSL9YonpwA"
      },
      "source": [
        "# Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0AtKvSCnsif"
      },
      "source": [
        "Hemos visto que es una buena práctica normalizar los datos de entrada. Este procedimiento también ayuda en las capas intermedias para reducir el problema de los gradientes que se desvanecen. En estas capas se conoce como Batch Normalization y se busca reescalar los datos de manera que cambie su media y desviación estándar.\n",
        "\n",
        "Un perceptrón multicapa sigue la siguiente estructura:\n",
        "\n",
        "$$X^l=f^l(X^{l-1}A^l+B^l)$$\n",
        "\n",
        "De aquí vemos que hay 3 formas de atacar al problema de los gradientes que se desvancen.\n",
        "\n",
        "\n",
        "*   Funciones de Activación $f^l$ que no se saturan (ReLU,SELU)\n",
        "*   Inicialización de $A^l$ y $B^l$ (Glorot, He)\n",
        "*   Normalización de $X^l$ (Batch Normalization)\n",
        "\n",
        "En sesiones anteriores se discutieron las 2 primeras alternativas y en este caso se analizará el Batch Normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aoMVPJ4wTh6"
      },
      "source": [
        "El primer paso consta en normalizar los datos:\n",
        "\n",
        "$$X^l\\to \\frac{X^l-\\mu^l}{\\sigma^l}$$\n",
        "\n",
        "Dónde $\\mu^l$ y $\\sigma^l$ son el promedio y la desviación estándar por lote de datos en la capa $l$. De esta manera, los datos en la capa $l$ quedaran normalizados con media nula y desviación estándar unitaria. Durante el entrenamiento deben acumularse los valores de $\\mu^l$ y $\\sigma^l$ de todos los lotes de datos para usarlos a la hora de realizar inferencia.  De aquí salen 2 parámetros por cada nodo (promedio y desviación) que se deben tener en cuenta. Sin embargo, estos parámetros no se optimizan usando gradiente descendente sino que se calculan por lotes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trsPeyqKxsR1"
      },
      "source": [
        "No es suficiente estandarizar los datos con media nula y varianza unitaria. Es posible que la red neuronal aprenda una normalización más eficiente. Es por esto que los datos se reescalan nuevamente de la siguiente manera:\n",
        "\n",
        "$$X^l\\to \\gamma \\frac{X^l-\\mu^l}{\\sigma^l}+\\beta$$\n",
        "\n",
        "Dónde $\\beta$ y $\\gamma$ serán el nuevo promedio y desviación estándar de los datos. Estos parámetros si debe aprenderlos la red neuronal durante el entrenamiento, por lo que tenemos 2 parámetros entrenables adicionales por cada nodo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLE1r1sG0VkW"
      },
      "source": [
        "Implementemos el Batch Normalization en Keras para el caso del dataset de Iris"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZxBasYF0SXH"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization #Importar BatchNorm\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWK6mQ8x0mgN"
      },
      "source": [
        "Se importan los datos, se normalizan y se hace un one-hot encoding de los labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf1tQxxs0lrk"
      },
      "source": [
        "data=load_iris()\n",
        "X,y=data.data,data.target\n",
        "X-=X.mean(axis=0)\n",
        "X/=X.std(axis=0)\n",
        "y=to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAUe26xM1wcM"
      },
      "source": [
        "Se construye una red neuronal con 4 capas y Batch Normalization después de la segunda capa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDkbWOeunrtZ"
      },
      "source": [
        "modelo=Sequential()\n",
        "modelo.add(Dense(4,activation='relu',input_shape=(4,)))\n",
        "modelo.add(Dense(4,activation='relu'))\n",
        "modelo.add(BatchNormalization())\n",
        "modelo.add(Dense(4,activation='relu'))\n",
        "modelo.add(Dense(3,activation='softmax'))\n",
        "modelo.compile('sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "modelo.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1L24Pou139V"
      },
      "source": [
        "En la capa de Batch Normalization hay 16 parámetros pero sólo 8 de esos son entrenables. Los 8 parámetros entrenables son los cuatro $\\gamma$ y cuatro $\\beta$ por cada neurona. Los 8 parámetros **no**-entrenables son los promedios $\\mu^l$ y las desviaciones estándar $\\sigma^l$ por cada nodo. Por último se entrena el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vz6tC1O0lDo"
      },
      "source": [
        "modelo.fit(x=X,y=y, batch_size=10,epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwbw0_D72fia"
      },
      "source": [
        "# Gradient Clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qnT0UtL2h1N"
      },
      "source": [
        "Aparte de los gradientes que se desvanecen, es posible que al hacer Backpropagation, los gradientes se acumulen de tal manera que resulten valores computacionalmente grandes. Este problema se conoce como Gradientes que Explotan y para lidiar con él debe limitarse el valor del gradiente. Esta solución es mucho más sencilla que los gradientes que explotan y sólo implica darle estabilidad numérica al gradiente. En keras hay 2 formas de hacerlo:\n",
        "\n",
        "\n",
        "*   Clip Norm: Se reescala todo el gradiente para que su norma no supere ciero umbral ($U$). Esta opción tiene la ventaja de conservar la dirección del gradiente:\n",
        "$$\\nabla J \\to \\nabla J \\frac{U}{|\\nabla J|}$$\n",
        "*   Clip Value: Se reescalan únicamente los valores que superen cierto umbral ($U$).\n",
        "$$(\\nabla J)_i \\to (\\nabla J)_i \\frac{U}{|\\nabla J|_i}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RHU0AyF3SZ-"
      },
      "source": [
        "El Gradient Clipping se implementa en el optimizador. Debido a esto debemos importar el optimizador explícitamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pB9ABS2VMe"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD #Importar el optimizador\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nixdAhmg3jEd"
      },
      "source": [
        "Nuevamente se importan, se estandarizan y se codifican los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_HAVEQB3RcK"
      },
      "source": [
        "data=load_iris()\n",
        "X,y=data.data,data.target\n",
        "X-=X.mean(axis=0)\n",
        "X/=X.std(axis=0)\n",
        "y=to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AXWe41o3m1i"
      },
      "source": [
        "Se construye el modelo con 3 capas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw9l1RuD3ZPB"
      },
      "source": [
        "modelo=Sequential()\n",
        "modelo.add(Dense(4,activation='sigmoid',input_shape=(4,)))\n",
        "modelo.add(Dense(4,activation='sigmoid'))\n",
        "modelo.add(Dense(4,activation='sigmoid'))\n",
        "modelo.add(Dense(3,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zz3G7gh3tgU"
      },
      "source": [
        "El optimizador hace el gradient clipping con el argumento \"clipnorm\" o \"clipvalue\" dependiendo del modo que se elija:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b13iXsw53s1C"
      },
      "source": [
        "optim=SGD(clipnorm=1) #Se cortan los gradientes estableciendo un umbral máximo para la norma\n",
        "modelo.compile(optim,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "modelo.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYMC5ntG34nv"
      },
      "source": [
        "Por último, se entrena la red:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAvgCDth3cAK"
      },
      "source": [
        "modelo.fit(x=X,y=y, batch_size=10,epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxPkG1iFoap4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}