{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S10_Optimizadores_Retroproyeccion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOcNElJbrtRxLNeFc8zFTTV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/S10_Optimizadores_Retroproyeccion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thBYXFC53-lF"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"140px\" src=\"https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1> Curso Deep Learning: Economía</h1>\n",
        "\n",
        "## S10: Optimizadores y algoritmo de retroproyección."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05VQD_CNrx36"
      },
      "source": [
        "# Optimizadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Kr2EoxAAqN"
      },
      "source": [
        "\n",
        "\n",
        "Vimos que la técnica de propagación hacia atrás utiliza el gradiente de descenso para ealizar la optimización de los pesos en cada pasada.\n",
        "\n",
        "La actualización mediante el gradiente de descenso lo hemos definico como:\n",
        "\n",
        "$$w_{t}=w_{t-1}-\\eta  \\nabla C(w)$$\n",
        "\n",
        "Donde $\\eta$ es llamado la taza de aprendizaje, éste debe escogerse de tal forma que no sea tan pequeño como para hacer muy lenta la convergencia, ni muy grande como para hacer que nuestros pesos diverjan.\n",
        "\n",
        "Uno de los  mayores incovenientes con éste algoritmo es que en el caso de las funciones de coste no convexas es probable quedar atrapado en un mínimo local, y jamás llegar a un mínimo global, para mitigar dicha posibilidad se han creado algunas variaciones como el gradiente de descenso estocástico o el gradiente de descenso por minibatch.\n",
        "\n",
        "### Gradiente de descenso estocástico.\n",
        "\n",
        "Como es planteado el gradiente de descenso solo hace un recalculo de los parámetros del modelo una vez ha hecho un paso completo de todos los ejemplos, ésto hace que el algorítmo sea de lenta convergencia y en caso de tener muchos ejemplos de entrenamiento pueda desbordar la memoria de la máquina.\n",
        "\n",
        "En el gradiente estocástico se realiza una actualización cada que se ha entregado un ejemplo a la red, con eésto se consigue una mayor varianza y por tanto las actualizaciones fluctuan en intensidad y es más difícil que el algoritmo quede atrapado en un mínimo local si éste no es bastante \"profundo\" lo cual garantiza mínimos más estables.\n",
        "\n",
        "### Gradiente de descenso por minibatch.\n",
        "\n",
        "Una de las desventajas del SDG reside en que al hacer una actualización por ejemplo puede ser bastante lento, una solución intermedia es pasar unos cuantos ejemplos antes de hacer la actualización (entregar un batch), con ello aumentamos la velocidad de entrenamiento pero no perdemos la ventaja de generar varianza sobre las actualizaciones para evitar ser atrabados en un mínimo local.\n",
        "\n",
        "Todos éstos métodos utilizan la mísma ecuación y la mísma idea de fondo, se diferencian en la cantidad de ejemplos pasados por la red antes de realizar la actualización de los pesos.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W6_pqeeFBAQ"
      },
      "source": [
        "## Momentum.\n",
        "\n",
        "Dado que los métodos de descenso de gradiente calculan las variaciones en todas direcciones suelen oscilar de forma innecesaria, lo cual hace que su convergencia sea más lenta. Para solucionar ésto se usa la técnica de momentum, en analogía con el momentum de la física clásica.\n",
        "\n",
        "Cuando una pelota rudea por una pendiente su velocidad aumenta en la dirección de movimiento (su derivada), mientras que en las demás direcciones no lo hace. En el método se sigue la misma idea, la actualización se hará en las direcciones en las que el gradiente aumentó más la actualización pasada, así nos aseguramos de no actualizar (demasiado) los pesos de manera innecesaria en direcciones que no contribuyen a la busqueda del mínimo.\n",
        "\n",
        "Si ahora notamos el paso de actualización de los pesos como: $$w_t = w_{t-1}-V(t)$$\n",
        "\n",
        "El $V$ en la iteración $t$ estará dado por:\n",
        "\n",
        "$$V(t) = \\gamma V(t-1)+\\eta \\nabla (w)$$\n",
        "\n",
        "Ahora tenémos un hiperparámetro nuevo $\\gamma$ que se encargará de determinar qué tanto influencia la actualización pasada a la actual, generalmente será un número al rededor de $0.9$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y92HHTXWJnaS"
      },
      "source": [
        "## Gradiente acelerado de Nesterov.\n",
        "\n",
        "En los años 80 el investigador Yurii Nesterov se dio cuenta que el momentum tenía un problema, al igual que en la analogía de la pelota, al llegar al mínimo el momento remanente empuja la pelota un poco más arriba del mínimo, también pasará lo mismo en el caso del error de la red, ésto puede demorar la convergencia o incluso hacer que el error no converja nunca. \n",
        "\n",
        "Para solucionar dícho problema Nasterov propuso no solo usar la influencia de la actualización pasada en la actualización presente si no que también se tendría en cuenta para el cálculo del gradiente (similar a tener en cuenta la aceleración para determinar que tan cerca se está de un cambio de direción).\n",
        "\n",
        "Así tenemos que:\n",
        "\n",
        "$$V(t) = \\gamma V(t-1)+\\eta \\nabla (w- \\gamma V(t-1))$$\n",
        "\n",
        "Por tanto la actualización disminuirá su \"momentum\" al acercarce al mínimo y por tanto no se alejará mucho de él."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YVhjysyMGKx"
      },
      "source": [
        "## Adagrad y Adadelta.\n",
        "\n",
        "Adagrad (adaptative gradient) nos permite actualizar de manera adaptativa cada parámetro, haciendo grandes modificaciones a parámetros infrecuentes mientras que lo hará con más cuidado a parámetros frecuentes. Ésto será útil cuando tratemos con datos dispersos (sparse data). \n",
        "\n",
        "Cada parámetro $w_i$ será actualizado así:\n",
        "\n",
        "$$w_{i,t} = w_{i,t-1}-\\frac{\\eta}{\\sqrt{G_{ii}+\\epsilon}}\\frac{\\partial C }{\\partial  w_i}$$\n",
        "\n",
        "El termino $G_{ii}$ es la acumulación de los gradientes pasados, $\\epsilon$ es un número pequeño para evitar la divergencia al dividir por cero.\n",
        "\n",
        "Un problema con ésta técnica es el hecho de que al acumular los gradientes en el denominador poco a poco el gradiente se hace más y más suave (también sufre de una especie de desvanecimiento) además de que el tiempo de entrenamiento también se hará mayor pues los pasos serán cada vez menores.\n",
        "\n",
        "Para atacar el problema del desvanecimiento usaremos **Adadelta**, en éste caso no tomaremos toda la acumulación de gradientes si no que se hará con una ventana de tamaño $\\rho$, definida por nosotros, con ello tenemos que el denominador será el RMS (sobre la vetana) de los gradientes, así:\n",
        "$$w_{i,t} = w_{i,t-1}-\\frac{\\eta}{RMS(\\frac{\\partial C }{\\partial  w_i})}\\frac{\\partial C }{\\partial  w_i}$$\n",
        "\n",
        "Ambas técnicas tienen la ventaja de no ser necesaria la definición de una razón de aprendizaje, ya que de manera iterativa ésta será calculada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCXsH10XT-X9"
      },
      "source": [
        "## Adam (Adaptative moment estimator).\n",
        "\n",
        "Adam calculará tanto el promedio de los gradientes pasados  (como Adadelta), así como el promedio del decaimiento de los graientes pasados (como momentum).\n",
        "\n",
        "Por tanto Adam es un estimador de momentos estadísticos adaptatico (en cada iteración). \n",
        "\n",
        "El primer momento acumulado será:\n",
        "$$\\hat{m} =\\frac{m_t}{1-\\beta_1^t}$$\n",
        "\n",
        "y el segundo:\n",
        "$$\\hat{v} =\\frac{v_t}{1-\\beta_2^t}$$\n",
        "\n",
        "y la actualización de los pesos será adaptada como:\n",
        "$$w_{i,t} = w_{i,t-1}-\\frac{\\eta}{\\hat{v}+\\epsilon}\\hat{m}$$\n",
        "\n",
        "$\\beta_i$ serpan hiperparámetros del optimizador.\n",
        "\n",
        "Adam es uno de los mejores optimizadores ya que es rápido, evita las fluctuaciones exageradas de los parámetros en direcciones no convenientes, y el desvanecimiento de los gradientes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTJWXNDfYYuK"
      },
      "source": [
        "![](https://miro.medium.com/max/620/1*XVFmo9NxLnwDr3SxzKy-rA.gif)\n",
        "\n",
        "![](https://miro.medium.com/max/620/1*SjtKOauOXFVjWRR7iCtHiA.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2l0Iis8Yurv"
      },
      "source": [
        "# Optimizadores en Keras.\n",
        "\n",
        "Como sabemos, uno de los pasos fundamentales en Keras para definir y entrenar un modelo es compilarlo, allí debemos decidir cual será el método de optimización a usar, además de la función de perdida y las métricas a usar.\n",
        "\n",
        "Los diferéntes optimizadores se importan como `from keras import optimizers` [https://keras.io/optimizers/](https://keras.io/optimizers/)\n",
        "\n",
        "Allí encontraremos los anteriores (y unos cuantos más).\n",
        "\n",
        "Podemos llamarlos en el compilador de dos maneras:\n",
        "\n",
        "\n",
        "\n",
        "*   Instanciandolos antes del compilador. \n",
        "*   Llamandolos dentro del compilador.\n",
        "Veamos:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc53ts34ZwL2"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,),activation='softmax'))\n",
        "#Instanciamos previo a llamarlo en el compilador, en éste caso podemos cambiar los parámetros del optimizador\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='mean_squared_error', optimizer=sgd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttIOaSEOZ6p4"
      },
      "source": [
        "#dando su nombre como parámetro al compilador, en éste caso los parámetros del optimizador usados serán sus valores por defecto\n",
        "model.compile(loss='mean_squared_error', optimizer='sgd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afdSfZ0HacJk"
      },
      "source": [
        "#Para llamar un gradiente estocástico\n",
        "#Recuerden que la difencia estará en cómo pasemos los bathc, además podemos darle un momentum o un nesterov,\n",
        "#SDG contiene entonces los optimizadores SDG, momentum, y Nesterov en una sola clase\n",
        "keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axk3lBtca93B"
      },
      "source": [
        "#Adagrad se llamará como:\n",
        "#el learning_rate es el valor inicial de dicho parámetro, \n",
        "#recuerde que éste método es adaptativo y éste valor cambiará con las iteraciones\n",
        "keras.optimizers.Adagrad(learning_rate=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdLjer0mbUWw"
      },
      "source": [
        "#Adadelta será:\n",
        "#El parámetro $\\rho$ controla la fracción de gradientes a tener en cuenta en cada paso de optimización.\n",
        "keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4QRdEvqb56R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "25605f99-1503-40ea-8e64-1a03c04b33eb"
      },
      "source": [
        "#finalmente Adam\n",
        "#donde amsgrad determina si se aplica una modificaión al método o no basada en https://openreview.net/forum?id=ryQu7f-RZ\n",
        "keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7f237ef009e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL5IIUOScN8Y"
      },
      "source": [
        "Recuerde que cada optimizador tendrá sus ventajas o desventajas dependiendo del problema, la función de coste y nuestro poder computacional.\n",
        "Por lo general el usdo de Adam o SDG con momentum serán una buena elección como primera prueba.\n",
        "\n",
        "Finalmete, en todos los optimizadores es posible usar los parámetros clipnorm (todos los valores de los gradientes serán recortados a una norma máxima) y clipvalue (todos los valores de los gradientes serán recortados a un valor mínimo de -n y máximo de n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90CbX_4xeQqy"
      },
      "source": [
        "#los valores de los gradientes estarán como máximo en una norma de 1\n",
        "sgd = optimizers.SGD(lr=0.01, clipnorm=1.)\n",
        "#los valores de los gradientes estarán mínimo en -0.5 y máximo en 0.5\n",
        "sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zCOCB-ayf2s"
      },
      "source": [
        "# Propagación hacia atrás (Back propagation)\n",
        "\n",
        "Aunque la propagacación hacia atrás se haga de manera automática en la mayoría de los paquetes computacionales es bueno que nos demos una idea de cómo funciona y su importancia dentro del DL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb3TMUa4yjxx"
      },
      "source": [
        "Éste método fue introducido a mediados de los años 70, pero no fue hasta el año 86, cuando un [artículo](http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf) publicado en la revista Nature por David Rumelhart, Geoffrey Hinton, and Ronald Williams; mostrando cómo su implementación en diferentes redes neuronales aumentava su velocidad de entrenamiento respecto a las técnicas usadas en la época, haciendo que problemas intratables hasta el momento se pudieran resolver con relativa fácilidad. Ésto hizo que la propagación hacia atrás se popularizara y que hasta ahora sea la técnica más usada en el entrenamiento de redes neuronales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEE2gVNbyr8q"
      },
      "source": [
        "La dificultad de hacer la actualización de los pesos de una red profunda radica en el hecho de que cada peso influye en todos los pesos de la capa subsiguiente (y por tanto en todos los demás de forma indirécta), ésto nos lleva a que sea una tarea computacionalmente costosa hacer una actualización de los pesos hacia delante pues una pequeña modificación hacia adelante puede tener consecuencias gigantes en las capas sucesivas.\n",
        "\n",
        "La propagación hacia atrás nos propone usar un método como el gradiente de descenso (iniciando con el error en la capa de salida) e ir propagando los errores hacia atrás como si cada capa fuera la salida de la anterior. Con ello podemos actualizar poco a poco y teniendo en cuenta los errores acumulados por la red. De forma efectiva estámos actualizando los pesos con toda la información disponible sin el riesgo de un efecto descontrolado en los pesos (como sucede en la propagación hacia adelante)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOON44Akyyw-"
      },
      "source": [
        "Son 4 las ecuaciones fundamentales del método:\n",
        "\n",
        "Denotaremos como $L$ la última capa, $l$ las capas ocultas, $j$ una neurona de la capa $l$, $\\sigma$ las funciones de activación de las capas ocultas y $a_j$ es la salida $j$ de la red ($a$ sería el vector de todas las salidas predichas por la red) y $z_j^l$ es la entrada pesada a la función de activación de la capa $l$ en la neurona $j$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Una ecuación para el error de la capa de salida:\n",
        "$$\\delta_j ^L = \\frac{\\partial C}{\\partial a_j}\\sigma(z_j^L)$$\n",
        "*   Una ecuación para el error de una capa ocualta dada por el error de la capa siguiente (error propagado hacia atráss):\n",
        "$$\\delta^l = ((w^{l+1})^T \\delta^{l+1})\\sigma(z^l)$$\n",
        "*   Una ecuación para la derivada de la función de costo para cualquier bias en la red:\n",
        "$$\\frac{\\partial C}{\\partial b_j^l}= \\delta_j^l$$\n",
        "*    Una ecuación para la derivada parial de la función de costo respecto a cualquier peso de la red:\n",
        "$$\\frac{\\partial C}{\\partial{w_{jk}^l}}=a_k^{l-1}\\delta_j^l$$\n",
        "\n",
        "\n",
        "Con las dos primeras ecuaciones nos es posible calcular el error en cualquier neurona, partiendo desde la capa de salida con la primer ecuación y llendo hacia atrás (propagando el error) cuantas veces sea necesario con la segunda ecuación.\n",
        "\n",
        "Con ello y las dos últimas es posible calcular las derivadas del costo en cualquier punto de la red y aplicando el gradiente de descenso tendríamos nuestra actualización de pesos de una forma más rápida y eficiente que con el uso de inversiones matriciales.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErTbw_G8y5x1"
      },
      "source": [
        "Con las ecuaciones a mano es fácil imaginar el algoritmo necesario para realiazar la propagación hacia atrás:\n",
        "*  Cree la activación para la capa de entrada $a_1$.\n",
        "*  Propague hacia adelanta para cada $l= 2,3,...,L$ calculando $z^l = w^la^{l-1}+b^l$ y $\\sigma(z^l)$\n",
        "*   Calcule el error de la capa de salida $\\delta^L$\n",
        "*   Propague hacia atrás para cada $l=l-1, l-2...2$ calculando el correspondiente $\\delta^l$\n",
        "*  Calcule los gradientes correspondientes a cada peso y cada bias y aplique el método de descenso de gradiente para actualizar cada peso.\n",
        "\n",
        "Si bien ésta es algorítmo detrás del método no es necesario aprender los detalles o las ecuaciones, pues éste ya viene integrado en las bibliotecas que usamos para realizar machine learning como Scikitlearn o Keras, sin embargo si es importante comprender su uso y porqué es tan importante y usado por los científicos de datos."
      ]
    }
  ]
}