{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_S07_Funciones_Activacion_Arquitectura_Red.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/DL_S07_Funciones_Activacion_Arquitectura_Red.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMTYh2DqhKFJ"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"140px\" src=\"https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1> Curso Deep Learning: Economía</h1>\n",
        "\n",
        "## S07: Arquitectura de Red y Funciones de activación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9uXGH6Vazft"
      },
      "source": [
        "# Funciones de activación:\n",
        "\n",
        "El proceso que describimos de los pasos de una neurona consisten en:\n",
        "\n",
        "- Entrada de los datos, o vector de características.\n",
        "\n",
        "- Ajuste lineal de los datos.\n",
        "\n",
        "- Mapeo del ajuste a una función no lineal.\n",
        "\n",
        "Gráficamente, los pasos anterior se ilustran en la siguiente figura:\n",
        "\n",
        "\n",
        "<img alt=\"Colaboratory logo\" height=\"220px\" src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/nn_rep.png?raw=true\" align=\"center\">\n",
        "\n",
        "En este ejemplo, se ha definido en la neurona una función de activación sigmoide $\\sigma(z)$. En red multicapas, el \n",
        "problema se ilustra gráficamente como:\n",
        "\n",
        "<img height=330px src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/msNN_v2.png?raw=true\" align=\"center\">\n",
        "\n",
        "en donde $a_i^{[l]}$ representa el resultado de aplicar la función de activación en la i-ésima neurona de la l-ésima capa. \n",
        "\n",
        "Teniendo en cuenta que una neurona consite en la composición de funciónes de un ajuste lineal y una activación, la salida de todos los ajuste de las neuronas de la capa oculta puede ser expresada en un vector de la forma:\n",
        "\n",
        "$$z^{[1]}=W^{[1]}x+b^{[1]}\\hspace{0.5cm}(1)$$\n",
        "\n",
        "y que corresponde al ajuste lineal de cada neurona de la capa (capa 1 en este caso), seguida de un activación:\n",
        "\n",
        "$$a^{[1]}=g^{[1]}(z^{[1]})\\hspace{0.5cm}(2)$$\n",
        "\n",
        "Similarmente, para una segunda capa, procesaría las activaciones $a^{[1]}$ generadas a la salida de la capa 1, nuevamente, realizando un ajuste lineal seguido de una activación. Matematicamente esto equivale a:\n",
        "\n",
        "$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\\hspace{0.5cm}(3)$$\n",
        "\n",
        "$$a^{[2]}=g^{[2]}(z^{[2]})\\hspace{0.5cm}(4)$$\n",
        "\n",
        "Estas cuatro ecuaciones caracterizan, escencialmente, a una red neuronal.\n",
        "\n",
        "Respecto al funcionamiento de la red y la función de activación asociada a una neurona nos surgen los siguientes interrogantes principales:\n",
        "\n",
        "1. ¿Cuál es la necesidad de introducir una función de activación por neurona?\n",
        "\n",
        "2. ¿Cuáles son las diferentes funciones de activación?\n",
        "\n",
        "3. ¿Que función de activación debemos usar en las diferentes capas?.\n",
        "\n",
        "4. ¿Cómo le indicamos a `keras` las funciones de activación por capas?."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF4H6K9OamGv"
      },
      "source": [
        "## 1. No linealidad en las redes neuronales: necesidad de funciones de activación.\n",
        "\n",
        "Veamos que pasa si asumimos una respuesta lineal de las reuronas (dada a partir de le función de activación) de una red neuronal; matemáticamente, esto quiere decir,\n",
        "\n",
        "$$g(z)=z \\hspace{0.5cm} (5)$$\n",
        "\n",
        "Introduciendo esta ecuación en las ecuación (2) tenemos que:\n",
        "\n",
        "$$a^{[1]}=z^{[1]}\\hspace{0.5cm}(2a)$$\n",
        "\n",
        "por lo que, de las ecuacion (1), (3) y (4) tenemos que: \n",
        "\n",
        "$$a^{[2]}=z^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}\\hspace{0.5cm}(3a)$$\n",
        "\n",
        "Reagrupando, llegamos a la siguiente expresión:\n",
        "\n",
        "$$a^{[2]}=z^{[2]}=\\underbrace{W^{[2]}W^{[1]}}_{W'}x+\\underbrace{(W^{[2]}b^{[1]}+b^{[2]})}_{b'}\\hspace{0.5cm}(3b)$$\n",
        "\n",
        "Por lo tanto la expresión (3b) se reduciría, nuevamente, a un ajuste lineal:\n",
        "\n",
        "$$a^{[2]}=z^{[2]}=W'x+b'$$\n",
        "\n",
        "Por lo tanto, si tenemos una red neuronal con muchas capas, todas con una función de activación lineal, o lo que es lo mismo, sin función de activación, colapsan a una sola neurona en la que se realiza un ajuste linea en donde el modelo no \"aprende\" nada nuevo a medida que adicionamos capas.\n",
        "\n",
        "La **función de activación** es la encargada de introducir un comportamiento no lineal, con el fin de poder \"aprender\" algo de los datos de entrada. En resumen, podemos afirmar que:\n",
        "\n",
        "Las funciones de activacción se utilizan para propagra la salida de los nodos de un capa hacia la siguiente capa. Las funciones de activación son funciones que van de los escalares a los escalares, produciendo la activación de la neurona. Las funciones de activación en las capas ocultas de las redes neuronales se usan con el fin de introducir una no-liniaridad en las capacidades de modelado del la red. \n",
        "\n",
        "De esta forma, hemos dado respuesta a la pregunta 1 sobre la necesidad de introducir dichas funciones de activación. Veamos ahora cuales son dichas funciones de activación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AwjaN6ZamGw"
      },
      "source": [
        "## Tipos de funciones de activación:\n",
        "\n",
        "### Lineal: \n",
        "\n",
        "Una transformación lineal, la cual se muestra en la siguiente figura, consiste, básicamente, en la función identidad, en donde la variable dependiente tiene una relación directa y proporcional con la variable independiente. En términos prácticos, lo anterior significa que una función de activación lineal pasa la señal sin realizar un cambio sobre esta.\n",
        "\n",
        "<img height=300px src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/linear_af.png?raw=true\">\n",
        "\n",
        "Este el típo de activación que se usa en la capa de entrada de las redes neuronales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUWXGgUamGx"
      },
      "source": [
        "### Sigmoide:\n",
        "\n",
        "Como todas las transformaciones logística, las sigmoides puede reducir la cantidad de valores extremos o outliers en los datos sin eliminarlos. En la siguiente figura se ilustra dicha función sigmoide:\n",
        "\n",
        "<img height=300px src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/sigmoid_af.png?raw=true\">\n",
        "\n",
        "Una función sigmoide convierde una variable independiente de rango infinito en probabilidades con un rango entre 0 y 1. La mayoria de las salidas serán cercanas a 0 o 1, que corresponden a zonas de saturación.\n",
        "\n",
        "La función de activación sigmoide devuelve una probabilidad independiente para cada clase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY0FiXdOamGy"
      },
      "source": [
        "### Tanh\n",
        "\n",
        "Corresponde a una función trigronométrica hiperbólica (ver siguiente figura), la cual representa la cual es igual a:\n",
        "\n",
        "$$\\text{tanh}(x) = \\frac{\\text{sinh(x)}}{\\text{cosh(x)}}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$\n",
        "\n",
        "<img height=300px src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/tanh_af.png?raw=true\">\n",
        "\n",
        "A diferencia de la función sigmoide, el rango de salida de tanh varía entre -1 y 1. La ventaja de tanh es puede tratar los números negativos de forma más eficiente que la función sigmoide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mh_Ezs0amGy"
      },
      "source": [
        "### ReLU: Rectified Linear Unit (Unidad Rectificadad Lineal)\n",
        "\n",
        "La unidad rectificada lineal es una trasformación más intersante que activa un nodo solo si la entrada se encuentra por encima de cierta cantidad. Si la entrada está por debajo de zero, la salida es cero, pero si la entrada alcanza un valor umbral, la salida tiene una relación lineal con la valriabe dependiente $\\text{max}(0,x)$, como se observa en la siguiente figura:\n",
        "\n",
        "<img height=350px src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/relu_af.png?raw=true\">\n",
        "\n",
        "Las funciones de activaciones ReLU represetan actualmente el estado de arte en la aplicación de funciones de activación, ya que han demostrado un buen desempeño en diferentes situaciones. Dado que el gradiente de la ReLU es ceero o constante, permite evitar el problema del desvanecimiento del gradiente. Las funciones de activación ReLu han demostrado un mejor entrenamiento en la práctica que las funciones de activación sigmoidea. \n",
        "\n",
        "Las funciones de activación ReLU se usan, normalmente, en las capas ocultas.\n",
        "\n",
        "Existen varias funciones de activación más, no obstante, nos centraremos a continuación en la descrisición de una función de activación usada en la capa de salida para la clasificación multiclase mediante una red neuronal, conocida como Softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXz0g9h_amGz"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "La función de activación Softmax representa una generalización de la regresión logística en el sentido que puede ser apllicada a un conjunto continues de datos (en lugar de una clasificación binaria) y puede contener múltiples fronteras de desición. Esta función maneja sistemas multinomiales para el etiquetado. Softmax es una función que normalmente se usa en la capa de salida de un clasificador.\n",
        "\n",
        "La función de activación Softmax devuelve la distribución de probablilidades de clases mutuamente excluyentes.\n",
        "\n",
        "Consideremos el siguiente ejemplo en donde se ilustra, entre otra cosas la definición de la función SoftMax:\n",
        "\n",
        "<img height=200px src=\"https://miro.medium.com/max/906/1*670CdxchunD-yAuUWdI7Bw.png\">\n",
        "\n",
        "En deep learning, el térno capa logit se usa para la última capa de neuronas de la red neuronal para el problema de clasificación que produce unos valores de predicción \"crudos\" de valores reales que varian en el intervalo $(-\\infty,\\infty)$. En concreto, las cantidades logits son los puntajes crudos de la ultima capa de la red neuronal, antes de que se les aplique la activación. \n",
        "\n",
        "Apliquemos definición de la función softmax para desarrollar una intuición de lo que esta significa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drfbER-CamG0"
      },
      "source": [
        "logits = [2.0, 1.0, 0.1] # Definición de los valores logits\n",
        "exps = [np.exp(i) for i in logits] # Calculo de los numeradores de la ecuación.\n",
        "\n",
        "sum_of_exps = sum(exps)# Cálculo del denominador\n",
        "\n",
        "softmax = [j/sum_of_exps for j in exps] # Cálculo del denominador\n",
        "\n",
        "softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQWGC06kamG4"
      },
      "source": [
        "sum(softmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_r1UbfeamG7"
      },
      "source": [
        "Si cada logit representa el punta en crudo asociado a una clase (grande, mediano, pequeño, por ejemplo), entoncees el valor sofmax asociado a cada clase representa la probabilidad o porcentaje de pertenencia a esa clase. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpdFl0p8amG8"
      },
      "source": [
        "## 4. Como usar las diferentes funciones de activación en keras:\n",
        "\n",
        "Para consultar las diferentes formas puede acceder a la documentación de kerras [aquí](https://keras.io/activations/).\n",
        "\n",
        "Existen formas formas para usar las funciones de activación en `keras`:\n",
        "\n",
        "1. A través de una capa, usando el método `Activation`. \n",
        "2. Mediante el argumento `activation` soportado por todas las capas posteriores.\n",
        "3. Mediante una función `TensorFlow/Theano/CNTK` por elementos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqX2KW5lamG9"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2WA-booamHC"
      },
      "source": [
        "# 1: A través de una capa medidante el método Activación:\n",
        "modelA = keras.models.Sequential()\n",
        "modelA.add(keras.layers.Dense(32, input_shape=(16,)))\n",
        "modelA.add(keras.layers.Dense(64))\n",
        "modelA.add(keras.layers.Activation('tanh'))#  Capa activación\n",
        "modelA.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD6O6ZSgamHG"
      },
      "source": [
        "# Mediante el argumento activation\n",
        "modelB = keras.models.Sequential()\n",
        "modelB.add(keras.layers.Dense(32, input_shape=(16,)))\n",
        "modelB.add(keras.layers.Dense(64, activation='tanh'))\n",
        "modelB.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmcOlZrWamHM"
      },
      "source": [
        "# Mediante una función de Tensor-flow por elementos.\n",
        "from keras import backend as K\n",
        "modelC = keras.models.Sequential()\n",
        "modelC.add(keras.layers.Dense(32, input_shape=(16,)))\n",
        "modelC.add(keras.layers.Dense(64, activation=K.tanh))\n",
        "modelC.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OToeFvdaamHS"
      },
      "source": [
        "### Funciones de activación principales en `Keras`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dz39t-camHS"
      },
      "source": [
        "# Relu\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x=np.random.uniform(low=-10.0, high=10, size=(2000,))\n",
        "y=keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)\n",
        "\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkBIZg35amHV"
      },
      "source": [
        "# tanh\n",
        "\n",
        "y=keras.activations.tanh(x)\n",
        "\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2egi0PQamHY"
      },
      "source": [
        "#sigmoid\n",
        "\n",
        "y=keras.activations.sigmoid(x)\n",
        "\n",
        "plt.plot(x,y,'.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F8jkHMRamHa"
      },
      "source": [
        "# elu\n",
        "y=keras.activations.elu(x, alpha=1.0)\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukX31wAamHe"
      },
      "source": [
        "# Exponential\n",
        "y=keras.activations.exponential(x)\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qldiTnfamHg"
      },
      "source": [
        "# Selu \n",
        "y=keras.activations.selu(x)\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9THrVDXamHi"
      },
      "source": [
        "# Linear\n",
        "y=keras.activations.linear(x)\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqeoGAw-amHl"
      },
      "source": [
        "y=keras.activations.softplus(x)\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtqTEOdEamHq"
      },
      "source": [
        "y=keras.activations.softsign(x)\n",
        "plt.plot(x,y,'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erS3n78auMP6"
      },
      "source": [
        "# Arquitectura y funcionalidad de la Redes neuronales secuenciales:\n",
        "\n",
        "De las funciones de activación habladas en la clase anterior podemos advertir dos características que deben poseer una red neuronal:\n",
        "\n",
        "1. Las funciones de activación de las capas ocultas deben ser funciones de activación no lineales, con el fín de que la red actue como un **aproximador universal a una función**.\n",
        "2. La función de activación de la capa de salida determina el tipo de clasificación/regresión del problema que se pretende solucionar.\n",
        "\n",
        "Como regla general, se tiene que la función de activación de las capas ocultas puede ser definida como una función `ReLU` y, dependiendo del problema, podemos definir la función de activación de la capa de salida como:\n",
        "\n",
        "* Función de activación sigmoide: si el problema de clasificación es binario.\n",
        "* Función de activación Softmax: si el problema de clasificación es multiclase.\n",
        "* Finción de activación lineal: si el problema se trata de una regresión.\n",
        "\n",
        "En resumen, en la siguiente figura se ilustran la arquitectura de red de los problemas que pueden presentarse en la clasificación/regresión usando una red neuronal secuencial y las funciones de activación definidas en las capas que la componen.\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/archi_clas_reg.png?raw=true\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "Veamos a continuación, someramente y sin entrar en detalles, los tres problemas de clasificación/regresión que pueden abordarse, a saber, la clasificación binaria, la clasificación multiclase y la regresión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2zp-dHAqiQP"
      },
      "source": [
        "## Problema\n",
        "\n",
        "Implementar un modelo de red neuronal usando Keras para predecir los sobrevivientes del titanic. El modelo debe tener dos capas ocultas densas con 16 neuronas cada una y funcion de activacion relu. Como optimizador usar descenso del gradiente estocástico como función de pérdida usar binary_crossentropy y como métrica usar accuracy. \n",
        "\n",
        "Nota: El dataset puede ser cargado de la siguiente dirección\n",
        "\n",
        "https://raw.githubusercontent.com/ssanchezgoe/curso_deep_learning_economia/main/data/DL_Titanic.csv\n",
        "\n",
        "Ayuda: Para resolver este problema siga los pasos enumerados a continuación:\n",
        "\n",
        "1. Cargue los datos en un dataframe de pandas.\n",
        "2. Analice la forma del dataframes y cuantos datos faltantes hay.\n",
        "3. Elimine del dataframe las variables que no guarden realación con la variable objetio (Survived). Tenga en cuenta que el dataframe estará compuesto de las siguientes características:\n",
        "\n",
        "  - PassengerIdUnique ID of the passenger\n",
        "   -SurvivedSurvived (1) or died (0)\n",
        "  - PclassPassenger's class (1st, 2nd, or 3rd)\n",
        "  - NamePassenger's name\n",
        "  - SexPassenger's sex\n",
        "  - AgePassenger's age\n",
        "  - SibSpNumber of siblings/spouses aboard the Titanic\n",
        "  - ParchNumber of parents/children aboard the Titanic\n",
        "  - TicketTicket number\n",
        "  - FareFare paid for ticket\n",
        "  - CabinCabin number\n",
        "  - EmbarkedWhere the passenger got on the ship (C - Cherbourg, S - Southampton, Q = Queenstown)\n",
        "\n",
        "4. Realice una imputación simple, usando como estrategia la media, de la variable 'age' diferenciada por supervivencia.\n",
        " - Para ello instancie un objeto `imputation` que sea `SimpleImputer(missing_values=np.nan, strategy='mean')`.\n",
        "5. Cree una nueva variable que se llame \"Family Size\", que vorresponda a la suma de las columnas 'SibSb' y 'Parch', y luego, elimine dichas columnas.\n",
        "6. Divida dicho dataset en una parte de entrenamiento y otra de test usando la función `train_test_split`\n",
        "7. Cree un modelo secuencial con la arquitectura requerida.\n",
        "8. Realice el entrenamiento de dicho modelo y grafique las curvas del puntaje de accuracy. "
      ]
    }
  ]
}