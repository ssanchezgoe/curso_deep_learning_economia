{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_S12_Regularizacion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1NI8i+1SnBa9XcSdQag6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/DL_S12_Regularizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMTYh2DqhKFJ"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"140px\" src=\"https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1> Curso Deep Learning: Economía</h1>\n",
        "\n",
        "## S12: Regularización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_SIflDDuwGN"
      },
      "source": [
        "# Librerías\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "QcGRPF6I6A9R"
      },
      "source": [
        "#@title Funciones a usar\n",
        "#@markdown plot_decision_boundary()\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    amin, bmin = X.min(axis=0) - 0.1\n",
        "    amax, bmax = X.max(axis=0) + 0.1\n",
        "    hticks = np.linspace(amin, amax, 101)\n",
        "    vticks = np.linspace(bmin, bmax, 101)\n",
        "    \n",
        "    aa, bb = np.meshgrid(hticks, vticks)\n",
        "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
        "    \n",
        "    c = model.predict(ab)\n",
        "    cc = c.reshape(aa.shape)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.contourf(aa, bb, cc, cmap='bwr', alpha=0.2)\n",
        "    plt.plot(X[y==0, 0], X[y==0, 1], 'ob', alpha=0.5)\n",
        "    plt.plot(X[y==1, 0], X[y==1, 1], 'xr', alpha=0.5)\n",
        "    plt.legend(['0', '1'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrSgnzkfsg7t"
      },
      "source": [
        "# Regularización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ZexC4wk5e7"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Muchas veces, cuando entrenamos nuestro modelo y procedemos a validarlo, encontramos que éste no puede generalizar de una forma correcta, es decir puede presentar **sobreajuste**, **alta varianza** u  **overfitting**. Existen multiples razones por las cuales esto se dé, poca cantidad de datos, una mala elección del modelo y su complejidad, entre otros.\n",
        "\n",
        "También se puede dar el caso contrario en que el modelo que se cree sea muy general y, aunque el desempeño en el conjunto de entrenamiento y evaluación es similar, el puntaje de clasificación es muy bajo. En este caso, decimos que el modelo sufre de **subajuste**, **alto sesgo** o **underfitting**.\n",
        "\n",
        "La siguiente gráfica resume los dos problemas mencionados hasta ahora\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"200px\" src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-03-at-7.52.01-PM-e1522832332857.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "Una forma de *evitar el overfitting es regularizando nuestro modelo*, es decir poniendo ciertas restricciónes a los pesos $W$, de forma tal que se fuercen a tomar valores pequeños, lo cual hará que su distribución de valores más regular. Como en el caso de las regresiones lineales, una de las formas de regularizar un modelo consiste en añadir un término a la función de costo, el cual, de acuerdo a su, define el tipo de regularización (**Ridge, lasso, Elasticnet**); o bien deteniendo el entrenamiento cuando el error de validación alcanzaba el mínimo, **Early stopping**. El *Early stopping* corresponde a una forma de hallar el valor optimo del número de épocas para produccir un modelo sin sobreajuste.\n",
        "\n",
        "Veamos cómo se puede usar los diferentes tipos de regularización y cómo se puede realizar su implementación en redes neuronales profundas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9jmjMW4s_ul"
      },
      "source": [
        "## **Regularización L1 (Lasso) y L2() Ridge**\n",
        "\n",
        "Los métodos más simples de regularización son $l1$ y $l2$, los cuales consisten en añadir un término a nuestra función de costo, el cual dependerá de cual de las regularizaciones usemos. Así, se castigan los valores grandes de la matriz de pesos y se restringen a pequeños valores. De forma aproximada, la regularización tiene la siguiente forma:\n",
        "\n",
        "\\begin{equation}\n",
        "J(W,b)=\\frac{1}{m}\\sum_{i}^{m}L(\\hat{y}^{(i)},y^{(i)}) + \\text{regularización}(W)\n",
        "\\end{equation}\n",
        "\n",
        "Como vemos la regularización se realizará sobre los pesos de la red sin incluir el término de sesgo o inclinación $b$ (bias term), esto se debe a que la mayoría de valores se encuentran en $W$. Lo anterior no impide a que podamos hacer también una penalización sobre los términos de sesgo.\n",
        "\n",
        "Para el caso de regularizasción **Rigida o $l2$** tenemos la siguiente forma para el costo\n",
        "\n",
        "\\begin{equation}\n",
        "J(W,b)=\\frac{1}{m}\\sum_{i}^{m}L(\\hat{y}^{(i)},y^{(i)}) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}||W^{[l]}||^{2}_{F}\n",
        "\\end{equation}\n",
        "\n",
        "Donde el término de regularización es una suma sobre $l2$ norma, que consisten en elevar al cuadrado las componentes de la matriz de pesos, a esta forma de regularización también se le conoce como **weight decay**.\n",
        "\n",
        "De forma similar, para la regularización **lasso o $l1$** se tiene\n",
        "\n",
        "\\begin{equation}\n",
        "J(W,b)=\\frac{1}{m}\\sum_{i}^{m}L(\\hat{y}^{(i)},y^{(i)}) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}||W^{[l]}||\n",
        "\\end{equation}\n",
        "\n",
        "donde la suma se da sobre los valores absolutos de las componentes de la matriz de pesos. Como hemos mencionado la regularización busca disminuir y evitar el overfitting, el cual puede presentarse en parte debido a la complejidad de nuestro modelo (ver siguiente ilustración).\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"250px\" src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-1.53.35-AM.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "Como vemos $l1$ y $l2$ consisten en añadir terminos proporcionales a $\\lambda$ (un hiperparámetro), que de acuerdo a su valor va a restringir nuestros pesos a ciertos pequeños valores. Recordemos que \n",
        "\n",
        "\\begin{equation}\n",
        "Z=W^{T}X + b\n",
        "\\end{equation}\n",
        "\n",
        "Si se tiene pesos pequeños, esto implicará un $Z$ pequeño, debido a su proporcionalidad, y por lo tanto el efecto de la función de activación estará restringido a valores pequeños, lo cual podemos traducir como una disminución en la complejidad del modelo,que es lo buscado.\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"250px\" src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-1.53.40-AM.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "De nuevo, esta calibración será guiada por el hiperparámetro $\\lambda$, pues de no tener un valor correcto se puede traducir en underfitting, cuando lo que se busca es el mejor modelo posible.\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"250px\" src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-03-at-10.37.28-PM.png\" align=\"center\" hspace=\"200px\" vspace=\"0px\"></p>\n",
        "\n",
        "`keras` proporciona 3 formas de realizar regularización:\n",
        "\n",
        "1. A través del parámetro `l1`: suma absoluta de pesos.\n",
        "2. A través del parámetro `l2`: suma cuadrática de pesos.\n",
        "3. A través del parámetro `l1_12`: sima absoluta y cuadrática de pesos (*elastic-net*).\n",
        "\n",
        "\n",
        "Cuando usamos keras, una forma de llamar el regularizador es a través del parámetro de la capa `kernel_regularizer`, de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDJCH5DbsbYS"
      },
      "source": [
        "layerl1 = keras.layers.Dense(10, activation=\"relu\",kernel_initializer=\"he_normal\",\n",
        "                            kernel_regularizer=keras.regularizers.l1(0.01),bias_regularizer='l1')\n",
        "\n",
        "layerl2 = keras.layers.Dense(10, activation=\"relu\",kernel_initializer=\"he_normal\",\n",
        "                            kernel_regularizer=keras.regularizers.l2(0.01),bias_regularizer='l2')\n",
        "\n",
        "layerl1_l2 = keras.layers.Dense(10, activation=\"relu\",kernel_initializer=\"he_normal\",\n",
        "                            kernel_regularizer=keras.regularizers.l1_l2(0.01),bias_regularizer='l1_l2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS_WmJ8jx33k"
      },
      "source": [
        "El argumento de las funciones `keras.regularizers.l1()`, `keras.regularizers.l2()` y `keras.regularizers.l1_l2()` corresponde al valor $\\lambda$ descrito en las ecuaciones de regularización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fihUKOyczQOt"
      },
      "source": [
        "## Ejemplo guiado\n",
        "\n",
        "En este ejemplo se demostrará la regularización de pesos para reducir el overfitting en un problema de clasificación simple.\n",
        "\n",
        "**Problema:** Usaremos una clasificación binaria en un conjunto de datos definido por dos semilunas (una semiluna por cada clase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgCIBGTbyO7H"
      },
      "source": [
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "df = DataFrame(dict(x1=X[:,0], x2=X[:,1], label=y))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6790K5Hu01m9"
      },
      "source": [
        "Podemos usar el método `groupby` de pandas para agrupar los conjuntos por etiquetas (0 y 1) y graficar cada subgrupo con un color:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K624vaR-0Nxw"
      },
      "source": [
        "for key, group in df.groupby('label'):\n",
        "  print(key)\n",
        "  print(group)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QkWPgtT0eB-"
      },
      "source": [
        "colors = {0:'red', 1:'blue'}\n",
        "fig, ax = plt.subplots()\n",
        "df.groupby('label')\n",
        "for key, group in df.groupby('label'):\n",
        "    group.plot(ax=ax, kind='scatter', x='x1', y='x2', label=key, color=colors[key])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lSTxw6Q1XdA"
      },
      "source": [
        "Este problema de pruebas es adecuado porque no pueden ser separados por una línea recta, lo que requiere de un modelo no lineal.\n",
        "\n",
        "Nótese que se han generado 100 puntos de ejemplo, lo cual es un conjunto pequeño para una red neuronal y proporciona una mayor probabilidad de overfitting en los datos de entrenamiento, y un error mayor en el conjunto de prueba, lo cual representa un escenario adecuado para el uso de regularización. Además, los ejemplos cuenta con algo de ruido, lo que le da la oportunidad al modelo de aprender aspectos de la muestra que no son generalizables.\n",
        "\n",
        "Dividamos manualmente el conjunto de datos en train y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvDF9jMx1Oml"
      },
      "source": [
        "n_train = 30\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ2EKbU23Cjq"
      },
      "source": [
        "### Modelo con sobreajuste\n",
        "\n",
        "Ilustremos el caso de un modelo con sobreajuste a los datos.\n",
        "\n",
        "El modelo consistirá de: \n",
        "- 500 neuronas en la capa de entrada y una función de activación `relu`.\n",
        "- En la capa de salida se contará con una función de activación sigmoid y una neurona. \n",
        "- La función de perdida será de entropía cruzada binaria.\n",
        "- El optimizador a usar será `adam`.\n",
        "- La métrica `accuracy`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4LVrCvg232P"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(500, input_dim = 2, activation='relu'))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMxKku-43-jW"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv90XQdZ4jTr"
      },
      "source": [
        "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose = 0)\n",
        "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "print(f\"Puntaje de entrenamiento: {train_acc:.3f}\")\n",
        "print(f\"Puntaje de evaluación: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geAk7z4a5rc3"
      },
      "source": [
        "plot_decision_boundary(model, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc00K_2z5xqs"
      },
      "source": [
        "DataFrame(history.history).plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zW52jhn7Ybi"
      },
      "source": [
        "### Modelo con regularización de pesos:\n",
        "\n",
        "Se puede adicionar regularización de pesos a la capa de entrada con el fin de reducir el overfitting del modelo a los datos de entrenamiento y mejorar el desempeño en el grupo de test. \n",
        "\n",
        "Para la regularización usaremos la penalización en la norma $l2$ con un parámetro de regularización arbitrario $\\lambda = 0.001$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx4mNRjr6gqX"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "model_l2 = keras.Sequential()\n",
        "model_l2.add(keras.layers.Dense(500, input_dim = 2, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
        "model_l2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "model_l2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5xRwkAU9Zsc"
      },
      "source": [
        "model_l2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNj3GkBz9fud"
      },
      "source": [
        "history_l2 = model_l2.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n",
        "_, train_acc = model_l2.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model_l2.evaluate(testX, testy, verbose=0)\n",
        "\n",
        "print(f\"Puntaje de entrenamiento: {train_acc:.3f}\")\n",
        "print(f\"Puntaje de evaluación: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvNQrIg89rJt"
      },
      "source": [
        "plot_decision_boundary(model_l2, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t7yoCsf-irD"
      },
      "source": [
        "DataFrame(history.history).plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5dpTPIoBIYG"
      },
      "source": [
        "### Búsqueda del hiperparámetro óptimo $\\lambda$\n",
        "\n",
        "Una vez se confirma que la regurarización ayuda a disminuir el sobre ajuste al modelo, se puede probar diferentes valores de regularización.\n",
        "\n",
        "Es una buena práctica buscar algunos valores de $\\lambda$ entre 0 y 0.1.\n",
        "\n",
        "Definamos un ciclo for para hacer estas búsquedas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PP-K5SU_MHs"
      },
      "source": [
        "# valores para el gridsearh\n",
        "lambdas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "train_set, test_set = list(), list()\n",
        "\n",
        "for val in lambdas:\n",
        "  print(f\"Processing lambda: {val}\")\n",
        "  keras.backend.clear_session()\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(500, input_dim=2, activation='relu', kernel_regularizer=keras.regularizers.l2(val)))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # Ajuste del modelo\n",
        "  model.fit(trainX, trainy, epochs=4000, verbose=0)\n",
        "  # Evaluación del modelo\n",
        "  _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "  _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "  print('Param: %f, Train: %.3f, Test: %.3f' % (val, train_acc, test_acc))\n",
        "  train_set.append(train_acc)\n",
        "  test_set.append(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQFJKwTECpkk"
      },
      "source": [
        "plt.semilogx(lambdas, train_set, label='train', marker='o')\n",
        "plt.semilogx(lambdas, test_set, label='test', marker='o')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ldEvjGUJypP"
      },
      "source": [
        "print(lambdas)\n",
        "print(train_set)\n",
        "print(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxCZjkBmG5nY"
      },
      "source": [
        "np.argmin(np.array(train_set)-np.array(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVYkZfmDKrds"
      },
      "source": [
        "lambdas[np.argmin(np.array(train_set)-np.array(test_set))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwU55riaRHUY"
      },
      "source": [
        "## **Dropout**\n",
        "\n",
        "Es uno de lo métodos mas usados como regularizador en redes neuronales. Fue propuesto por Geoffrey Hinton en el  2012. Dropout consiste en que en cada paso de entrenamiento se quitarán ciertas neuronas de nuestra red, es decir se tendrá cierta probabilidad \"**P**\" de que una neurona sea quitada o apagada (hacerla cero) durante ese paso del entrenamiento, para esto se tendrá en cuenta la capa de entrada y se excluíra la capa de salida.\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"150px\" src=\"https://miro.medium.com/max/1354/1*skMXofkjeXtKzSr5lqIEmg.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "El hiperparámetro \"**P**\" es llamado **dropout rate**, normalmente tiene un valor entre $0.2$ y $0.5$, es decir, si tomamos 0.2, $1$ de $5$ unidades serán quitadas. Si ciertas neuronas en un paso son desactivadas, al siguiente no necesariamente lo estarán, pues de nuevo se tendrán en cuenta a la hora de hacer la desactivación en el nuevo paso del entrenamiento. Es importante aclarar que el **dropout** solo se realizará durante la etapa de entrenamiento, no en la de validación.\n",
        "\n",
        "Este método de regularización es altamente usado pues se comporta como un **ensemble**, es decir, como la unión de varios modelos, esto puede ser interpretado de esta manera, pues en cada paso del entrenamiento al tener diferentes neuronas desactivadas, esto \"equivale\" a tener diferentes redes neuronales. Su implementación en keras es la siguiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5E2nJn1T0L1"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "                  keras.layers.Flatten(input_shape=[28, 28]),\n",
        "                  keras.layers.Dropout(rate=0.2),\n",
        "                  keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "                  keras.layers.Dropout(rate=0.2),\n",
        "                  keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "                  keras.layers.Dropout(rate=0.2),\n",
        "                  keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs6-FucfRNhu"
      },
      "source": [
        "Donde en esta implementación vemos que se usó **keras.layers.Dropout**, donde durante el entrenamiento keras quita algunas de las entradas(haciendolas cero). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2rcHouwUYqW"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## **Regularización de norma máxima (Max-norm)**\n",
        "\n",
        "Es una forma de regularización muy usada en redes neuronales la cual consiste en restringir los valores de los pesos a cierto valor máximo, **r**, el cual es su hiperparámetro. En este caso no se le añade ningún término a la función de costo, en vez de eso se cálcula $|\\textbf{w}|_{2}$ en cada paso y si se supera el valor límite se reescalan los valores de la siguiente forma:\n",
        "\\begin{equation}\n",
        "\\textbf{w} \\leftarrow \\textbf{w}\\frac{r}{|\\textbf{w}|_{2}} \n",
        "\\end{equation}\n",
        "\n",
        "Reucir el **r** implica incrementar la cantidad de regularización, lo cual puede ayudar con el overfitting. Su implementación en Keras es en la siguiente manera"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUXTdESVsN75"
      },
      "source": [
        "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
        "                    kernel_constraint=keras.constraints.max_norm(1.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5OAxnq4s4s_"
      },
      "source": [
        "## **Early Stopping**\n",
        "\n",
        "Este  es un tipo de método de validación cruzada, el cual conciste en analizar el conjunto de validación y en el momento en que su error empiece a aumentar detener el entrenamiento. [callbacks](https://keras.io/callbacks/)\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"250px\" src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-12.31.56-AM.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "En keras se puede aplicar este método, haciendo uso de los *callbacks*, los cuales me permiten monitorear él codigo e interactuar con el proceso de entrenamiento de forma automática."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76a0iI_Cuy7k"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "EarlyStopping(monitor='val_err', patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WrYDYpYRtXq"
      },
      "source": [
        "# Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ymqX6qmsgT7"
      },
      "source": [
        "## Ejercicio 1:\n",
        "\n",
        "Entrene una red neuronal para el dataset iris y realice los siguiente pasos:\n",
        "\n",
        "1. Carge los datos y estandarícelos.\n",
        "2. Use la función de keras `to_categorical` para dummificar la variable $y$.\n",
        "1. 2 capas (contando la capa de entrada) de 16 y 8 neuronas cada una, función de activación ReLU e inicializadando los pesos con he_normal. \n",
        "2. Entrene la red usando el optimizador **Adam** y función de perdida `categorical_crossentropy`.\n",
        "\n",
        "3. Cree los siguientes callbaks y llamelos en el entrenamiento:\n",
        "\n",
        "```\n",
        "my_callbacks = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100),\n",
        "                keras.callbacks.EarlyStopping(monitor='accuracy', patience=100),\n",
        "                keras.callbacks.EarlyStopping(monitor='loss', patience=20)]\n",
        "```\n",
        "\n",
        "\n",
        " 4. Mediante el parámetro del método `fit` `validation_split`, usando un 20% como datos de validación. \n",
        " 5. Defina 1000 épocas de entrenamiento.\n",
        " 6. Fije el parámetro de `batch_size` en 16.\n",
        " 7. Entrene el modelo y guarde el entrenamiento en un objeto `history`.\n",
        " 8. Grafique las curvas de entrenamiento y validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVVyleaNSj4W"
      },
      "source": [
        "# Use estas funciones y librerías.\n",
        "from sklearn.datasets import load_iris\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R7B-ZdhveEZ"
      },
      "source": [
        "#Celda para carga de datos y estandarización."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5X9NcsXCngqO"
      },
      "source": [
        "#@title Si tiene problemas para cargar los datos y estandarizarlos, haga click en Mostrar código.\n",
        "data=load_iris()\n",
        "X,y=data.data,data.target\n",
        "X-=X.mean(axis=0)\n",
        "X/=X.std(axis=0)\n",
        "y=to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TjPrBylvjaK"
      },
      "source": [
        "#Celda para creación del modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jvRtVbkzSkXB"
      },
      "source": [
        "#@title Si tiene problemas para crear el modelo, haga click en Mostrar código.\n",
        "keras.backend.clear_session()\n",
        "modelo=Sequential()\n",
        "modelo.add(Dense(16,input_dim=4,activation='relu',kernel_initializer='he_normal'))\n",
        "modelo.add(Dense(8,activation='relu',kernel_initializer='he_normal'))\n",
        "modelo.add(Dense(3,activation='softmax',kernel_initializer='glorot_normal'))\n",
        "modelo.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iR272oxvm-w"
      },
      "source": [
        "#Definición de los callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "--iObMFKhzfU"
      },
      "source": [
        "#@title Si tiene  problema para definir los callbacks, haga click en Mostrar Código.\n",
        "my_callbacks = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100),\n",
        "                keras.callbacks.EarlyStopping(monitor='accuracy', patience=100),\n",
        "                keras.callbacks.EarlyStopping(monitor='loss', patience=20)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oC_foDvrZx"
      },
      "source": [
        "# Ajuste del modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "hfNoTrTuS4fs"
      },
      "source": [
        "#@title Si tiene problema para definir el ajuste del modelo, haga click en Mostrar código.\n",
        "history=modelo.fit(x=X, y=y, validation_split=0.2, epochs=1000, callbacks=my_callbacks, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZZ3eL3jwBcQ"
      },
      "source": [
        "# Gráfica"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gaeHZndwS78B"
      },
      "source": [
        "#@title Si tiene problemas para graficar, haga click en Mostrar código.\n",
        "DataFrame(history.history).plot(figsize=(10,10))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU8sBfzTsrIO"
      },
      "source": [
        "## Ejercicio 2:\n",
        "\n",
        "Realice el mismo ejercicio anterior agregando una capa de `dropout` del 20% entre las capas de 16 y 8 neuronas.\n",
        "\n",
        "¿Qué conclusión puede sacar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmfG9X4tydQP"
      },
      "source": [
        "# Celda para definición del modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ohleXpGytIuw"
      },
      "source": [
        "#@title Si tiene problema para definir el modelo, haga click en Mostrar código.\n",
        "keras.backend.clear_session()\n",
        "modelo=Sequential()\n",
        "modelo.add(Dense(16,input_dim=4,activation='relu',kernel_initializer='he_normal'))\n",
        "modelo.add(Dropout(0.2))\n",
        "modelo.add(Dense(8,activation='relu',kernel_initializer='he_normal'))\n",
        "modelo.add(Dense(3,activation='softmax',kernel_initializer='glorot_normal'))\n",
        "modelo.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgEyznWYyoqd"
      },
      "source": [
        "# Celda para definir los callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "E5gvGbystfl5"
      },
      "source": [
        "#@title Si tiene problemas para definir los callbacks, haga click en Mostrar código.\n",
        "my_callbacks = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100),\n",
        "                keras.callbacks.EarlyStopping(monitor='accuracy', patience=100),\n",
        "                keras.callbacks.EarlyStopping(monitor='loss', patience=20)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoRfk6_Ay2ED"
      },
      "source": [
        "# Celda para el entrenamiento del modelo."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cgTRqhCjtjgN"
      },
      "source": [
        "#@title Si tiene problema para entrenar el modelo, haga click en Mostrar código.\n",
        "history=modelo.fit(x=X,y=y,validation_split=0.2,epochs=1000, callbacks=my_callbacks, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9is9qjmazD_k"
      },
      "source": [
        "# Celda para graficar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6TfO5KQktl__"
      },
      "source": [
        "#@title Si tiene problemas para graficar, haga click en Mostrar código.\n",
        "DataFrame(history.history).plot(figsize=(10,10))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}