{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_S14_CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNndaJ6oZBT0IRCx9Q4QMEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/curso_deep_learning_economia/blob/main/NBs_Google_Colab/DL_S14_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMTYh2DqhKFJ"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"140px\" src=\"https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1> Curso Deep Learning: Economía</h1>\n",
        "\n",
        "## S14: Redes neuronales convolucionales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCheopcCYee4"
      },
      "source": [
        "# Redes neuronales convolucionales (CNN)\n",
        "\n",
        "Las redes neuronales convolucionales (CNN) surgieron del estudio de la corteza visual del cerebro, y se han utilizado en el reconocimiento de imágenes desde la década de 1980. En los últimos años, gracias al aumento en el poder computacional, la cantidad de datos de entrenamiento disponibles y los trucos presentados en la clase anteriro para entrenar redes profundas, las CNN han logrado alcanzar un rendimiento sobrehumano en algunas tareas visuales complejas. Impulsan servicios de búsqueda de imágenes, autos que se manejan solos, sistemas automáticos de clasificación de video y más. Además, las CNN no se limitan a la percepción visual: también tienen éxito en muchas otras tareas, como el reconocimiento de voz o el procesamiento del lenguaje natural (PNL).\n",
        "\n",
        "La diferencia fundamental entre una capa densamente conectada y una capa de convolución (convolutional layer ) es que las capas densas aprenden patrones globales en su espacio de características de entrada (por ejemplo, para un dígito MNIST, patrones que involucran todos los píxeles), mientras que las capas de convolución aprenden patrones locales, enn el caso de imágenes, patrones encontrados en pequeñas ventanas de las caracteristicas de entrada. \n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/qdvojdR.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n",
        "\n",
        "Esta característica clave otorga a las capas convolucionales dos propiedades interesantes:\n",
        "* Los patrones que aprenden son invariantes de translacion. Después de aprender un cierto patrón en la esquina inferior derecha de una imagen, una CNN puede reconocerlo en cualquier lugar, por ejemplo, en la esquina superior izquierda. Una red densamente conectada tendría que aprender el patrón nuevamente si apareciera en una nueva ubicación. Esto hace que los datos de las CNN sean eficientes cuando se procesan imágenes (porque el mundo visual es fundamentalmente invariante ante translacion).\n",
        "\n",
        "* Pueden aprender jerarquías espaciales de patrones: una primera capa de convolución aprenderá pequeños patrones locales como los bordes, una segunda capa de convolución aprenderá patrones más grandes hechos de las características de las primeras capas, y así sucesivamente. Esto permite que las CNN aprendan eficientemente conceptos visuales cada vez más complejos y abstractos (porque el mundo visual es fundamentalmente jerárquico espacialmente).\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/FaeuJas.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n",
        "\n",
        "Ahora que tenemos una idea de que son las CNN pasemos a ver el elemento que hace tan especiales a las CNN, esto es , las capas convolucionales. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxs5UBiGkA7x"
      },
      "source": [
        "## Convolutional layer\n",
        "Una capa convolucional tiene por lo general tres etapas como se muestra en la figura \n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"450px\" src=\"https://i.imgur.com/PyoAVmM.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n",
        "\n",
        "Vemos cada una de estas etapas con un poco mas de detalle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2c0sl4lqfcl"
      },
      "source": [
        "## Stage 1:  Convolución\n",
        "\n",
        "La convolución (en el contexto las redes neuronales convolucionales) es básicamente una correlación cruzada (sin embargo la seguiremos llamando convolución), la cual, en procesamiento de señales, es una medida de la similitud entre dos señales, frecuentemente usada para encontrar características relevantes en una señal desconocida por medio de la comparación con otra que sí se conoce.\n",
        "Matematicamente la correlacion cruzada (en el caso discreto) se defien como:\n",
        "\n",
        "\\begin{equation}\n",
        "S[t] = (X * W)(t) = \\sum_a X[a]W[t+a]\n",
        "\\end{equation}\n",
        "\n",
        "para entender mejor el concepto de convolución veamos un ejemplo simple usando dos señales. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUcB67sa1BaX"
      },
      "source": [
        "### Ejemplo : dos señales 1-dimensional\n",
        "\n",
        "Para ver de forma mas detalla como se hacen animacion con matplotlib en google colab puede visitar el link: https://colab.research.google.com/drive/1lnl5UPFWVPrryaZZgEzd0theI6S94c3X#scrollTo=OEwd0xc5eGz9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgf3qpdOA0EX"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "#import seaborn as sns; sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lobVsc-_A0CO"
      },
      "source": [
        "# Primero configure la figura, el eje y el elemento de la trama que queremos animar\n",
        "fig, ax = plt.subplots(figsize=(13,8) )\n",
        "plt.close()\n",
        "\n",
        "y1 = np.array([0.1,0.2,-0.1,4.1,-2,1.5,-0.1])\n",
        "x1=np.arange(1,len(y1)+1)\n",
        "ax.plot(x1,y1+7,'o-')\n",
        "\n",
        "ax.set_xlim(( -7, 15))\n",
        "ax.set_ylim((-3, 12))\n",
        "ax.set_yticks([])\n",
        "ax.set_xticks([])\n",
        "line, = ax.plot([], [], 'o-r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn1WGWOuAz_R"
      },
      "source": [
        "# función de inicialización: traza el fondo de cada cuadro\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    return (line,)\n",
        "\n",
        "# función de animación\n",
        "def animate(i):\n",
        "  i=i-6\n",
        "  y2 = np.array([0.1,4,-2.2,1.6,0.1,0.1,0.2])\n",
        "  x=np.arange(1,len(y2)+1)+i\n",
        "  line.set_data(x, y2)\n",
        "  ax.set_title('cross correlation=%.3f' %(np.correlate(y1,y2,mode='full'))[6+i], fontsize=20)\n",
        "  for t in ax.texts:\n",
        "    t.set_visible(False)\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    ax.text(x1[i], y1[i]+7, str(y1[i]))\n",
        "    ax.text(x[i], y2[i], str(y2[i]))\n",
        "\n",
        "    \n",
        "  return (line,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH6PkYAuAz9C"
      },
      "source": [
        "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                             frames=7+6, interval=2000, blit=True)\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aYp932C5kzF"
      },
      "source": [
        "como se puede observar , el valor más grande de correlación cruzada es $23.18$. Si comparamos la forma de las señales justo para este valor de correlación vemos que la forma de las señales es bastante parecida. Vemos entonces que la correlación cruzada es un indicador de la similitud de la forma ( de las características ) de estas dos señales.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6gF7kmyFty9"
      },
      "source": [
        "### Caso extendido \n",
        "\n",
        "En el contexto de las CNNs, estas señales son multidimensionales y no solo 1-dimensional como en el ejemplo que acabamos de ver. Consideremos por ejemplo el caso de una imagen 2-dimensional (supongamos una imagen a blanco y negro), para este caso nuestra ecuación para la convolución(correlación cruzada) la siguiente forma:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "S[i,j] = (I * K)[i,j] = \\sum_m \\sum_n I[m,n]W[m+i,n+j]\n",
        "\\end{equation}\n",
        "\n",
        "Donde en el contexto de las CNNs, la señal $K$ es conocida como Kernel. En la ecuación anterior estamos haciendo la convolución de la imagen $I$ con el Kernel $K$.\n",
        "\n",
        "veamos un ejemplo práctico de esto para entender un poco mejor qué es lo que sucede"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIis0_HRIF2X"
      },
      "source": [
        "#### Ejemplo: imagen 2-dimensional \n",
        "\n",
        "Consideremos la siguiente imagen y el sigueinte kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrG8TZo8IBAr"
      },
      "source": [
        "I = np.array([[1,1,1,0,0], [0,1,1,1,0], [0,0,1,1,1], [0,0,1,1,0], [0,1,1,0,0]])\n",
        "K = np.array([[1,0,1],[0,1,0],[1,0,1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL-3CzKeIA07"
      },
      "source": [
        "print(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqCliOIJms8"
      },
      "source": [
        "print(K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msirckA_J6CE"
      },
      "source": [
        "Ahora usemos la libreria de Scipy para realizar la convolucion "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpxbR9p5J-KO"
      },
      "source": [
        "from scipy.signal import convolve2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHwhXQotJ-KU"
      },
      "source": [
        "convolve2d(I,K, mode='valid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfdOi-IbRnwU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "![](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)\n",
        "\n",
        "Este tipo de convolución es es conocida como convolucion valid.\n",
        "\n",
        "Realizando esta operación de convolución entre $I$ y $K$, podemos capturar algunas de las características de la imagen que son similares a las de nuestro kernel. Para apreciar mejor esta idea , veamos un ejemplo con una imagen más grande.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF8eLoC07D71"
      },
      "source": [
        "#### Convolucion en imagen mas grande\n",
        "\n",
        "Por ahora consideremos una imagen a blanco y negro , sin embargo más adelante veremos que las imágenes pueden venir con más canales (pro ejemplo los ampliamente conocidos RGB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmwDcv-7Rm4m"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "from scipy.signal import convolve2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90leGk1T1SLn"
      },
      "source": [
        "image= mpimg.imread('https://i.imgur.com/R2mS8Oh.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZcLWRejHvfw"
      },
      "source": [
        "image.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFwgzYiAGjx6"
      },
      "source": [
        "K=np.array([[1,1,1],\n",
        "           [0,0,0],\n",
        "           [-1,-1,-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k02TOOlnCMxQ"
      },
      "source": [
        "fig , ax = plt.subplots(2,3, figsize=(18,15), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.05, wspace=0.1))\n",
        "ax[0,0].imshow(image, cmap='Greys' )\n",
        "ax[0,1].imshow(K,cmap='Greys')\n",
        "ax[0,2].imshow(K.T,cmap='Greys')\n",
        "ax[1,1].imshow(convolve2d(image,K, mode='valid'), cmap='Greys')\n",
        "ax[1,2].imshow(convolve2d(image,K.T, mode='valid'), cmap='Greys')\n",
        "ax[1,0].imshow(convolve2d(image,K.T, mode='valid')+convolve2d(image,K, mode='valid'), cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwhbCgRrNQV8"
      },
      "source": [
        "En las CNNs los kernels son aprendidos por nuestro algoritmo, es decir,  basados en las predicciones que queremos realizar , le decimos a nuestro modelo que encuentre cuál es el mejor kernel para dicha tare.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZQHuTsO1J_"
      },
      "source": [
        "### Efectos de borde, y strides\n",
        "como se vio en el ejemplo anterior, después de realizar la operación de convolución la imagen redujo su tamaño. En general, el ancho y alto de la imagen de salida pueden diferir del ancho y alto de la imagen de entrada debido a dos razones: \n",
        "\n",
        "*  Efectos de borde, que se pueden contrarrestar usando el metodo llamado padding.\n",
        "\n",
        "*  El uso de strides\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ_znHQURfLy"
      },
      "source": [
        "#### Efectos de borde y padding \n",
        "\n",
        "Consideremos el ejemplo de nuestra imagen de 5x5 píxeles, y un kernel de 3x3.\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/uyepS9e.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n",
        "\n",
        "\n",
        "si queremos que el tamaño de nuestra imagen de salida (esto es, después de aplicar la operación de convolución con el kernel) tenga el mismo tamaño de nuestra imagen de entrada podemos usar la técnica llamada padding. Padding consiste en agregar un número apropiado de filas y columnas a cada lado de la imagen para que así la imagen de salida tenga las mismas dimensiones de la imagen de entrada. Para un kernel 3×3, agrega una columna a la derecha, una columna a la izquierda, una fila en la parte superior y una fila en la parte inferior.\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"250px\" src=\"https://i.imgur.com/p1MdvHF.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n",
        "\n",
        "La convolución que consta de padding y luego convolución , es conocida como convolución “Same”. Veamos un ejemplo de esto usando scipy \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5hh7LgEQCaX"
      },
      "source": [
        "convolve2d(I,K, mode='same')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDGUWILOWH5G"
      },
      "source": [
        "Usualmente el padding se realiza agregando columnas y filas de ceros\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"350px\" src=\"https://i.imgur.com/5BoEHiY.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr07c-9IWotZ"
      },
      "source": [
        "#### Strides \n",
        "\n",
        "Como vimos en el ejemplo de la imagen 5x5 , cuando desplazamos nuestro kernel en la imagen para realizar la operación de convolución , nos desplazamos solo una columna o solo una fila, sin embargo estos desplazamientos son un parámetro de la convolución llamado stride y en general puede ser diferente de uno. \n",
        "<p><img alt=\"Colaboratory logo\" height=\"350px\" src=\"\n",
        "https://i.imgur.com/vMGjDk9.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "Debido a fijar los stides diferente de uno , la imagen puede verse reducida después de la operación de convolución.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55yXg5NYWr9O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1RJDDvSbui_"
      },
      "source": [
        "## Stage 2: Detector Stage (activation stage)\n",
        "\n",
        "Esta etapa es similar a la que ya conocemos de las DNN, se trata de aplicar una transformación no lineal (funciones de activación) tales como Relu, Tanh, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whxEzgflfk3-"
      },
      "source": [
        "## Stage 3: Pooling\n",
        "\n",
        "En la etapa de Polling vamos a calcular un resumen estadístico de nuestra imagen una vez a pasado por las dos etapas anteriores ( esto es , convolution  y detector stage). Hay varias razones para realizar esto: \n",
        "\n",
        "* Reducir la imagen de entrada para reducir la carga computacional, el uso de memoria y el número de parámetros (lo que limita el riesgo de overfitting).\n",
        "\n",
        "* Introducir cierto nivel de invariancia a pequeñas traslaciones.\n",
        "\n",
        "Hay diferentes formas de hacer pooling , entre las más conocidas están Max Polling ( The maximum of a rectangular neighborhood) \n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/BW48gCv.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "Ahora tenemos todas las herramientas para construir nuestras CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc_Qayl7l2IT"
      },
      "source": [
        "## Forma mas gerenal \n",
        "Hasta ahora, por simplicidad, hemos representado la salida de cada capa convolucional como una delgada capa 2D, pero en realidad una capa convolucional tiene múltiples Kernels (filtros), y genera un mapa de características por filtro, por lo que es representado con mayor precisión en 3D.\n",
        "\n",
        "Además, las imágenes de entrada también se componen de múltiples subcapas: una por canal de color. Normalmente hay tres: rojo, verde y azul (RGB). Las imágenes en escala de grises tienen solo un canal, pero algunas imágenes pueden tener mucho más, por ejemplo, imágenes satelitales que capturan frecuencias de luz adicionales (como infrarrojo).\n",
        "<p><img alt=\"Colaboratory logo\" height=\"400px\" src=\"https://i.imgur.com/OuDTED7.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "## La arquitectura tipica de las CNN es como se muestra en la figura de abajo \n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/BqlLRkJ.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIursD94ubwb"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4PR70auui8c"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNHamTJ0ha-A"
      },
      "source": [
        "### **Implementación de una red convolusional en Keras**\n",
        "La construcción de nuestra red se da de manera similar a como se ha venido realizando, solo que para este caso debemos añadir capas convolucionales, es decir que realicen el proceso de convolución. Keras cuenta con diferentes tipos de capas convolucionales, esta será definida de acuerdo a nuestras necesidades, estás se pueden ver en el siguiente [link](https://keras.io/layers/convolutional/). \n",
        "\n",
        "Veamos como sería la implementación para el caso de una **convolución2D**. Al crear nuestra red se añade la capa de la siguiente manera:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R57CGjPj7Lc"
      },
      "source": [
        "mod=keras.models.Sequential([\n",
        "      keras.layers.Conv2D(filters=64,kernel_size=3,strides=(1,1),padding='valid',activation='relu',use_bias=True,kernel_initializer='he_uniform', bias_initializer='zeros')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRdPBI94mtyU"
      },
      "source": [
        "En la línea de código anterior de forma didáctica se construyó una capa convolucional en la forma que se hace en keras, allí solo tuvimos en cuenta algunos argumentos. Esta capa crea un kernel de convolución que se convoluciona  (valga la redundancia) con capa de entrada para producir un tensor como salida. Dado el caso de que esta sea nuestra capa de entrada, recordemos debemos entrar la forma de neustros datos, donde este será la forma de neustra matriz más la componente que nos dará el número de canales que se tenga. \n",
        "\n",
        "Los argumentos en la anterior capa hacen referencia a :\n",
        "* **Filters:** Entrada para el número de filtro que usará nuestra capa, valor entero.\n",
        "* **kernel_size:** Recibe una tupla como entrada en la que le daremos el valor de la altura y el ancho de nuestro filtro (kernel), si le damos un entero como en este caso, keras interpretará como que tiene ese mismo valor en la altura y el ancho.\n",
        "* **Strides:** Recibe una tupla como entrada en la que específicamos los strides, en dirección horizontal y vertical.\n",
        "\n",
        "* **Padding:** recibe como argumento 'valid' o 'same', donde en la primera no tendremos padding y con la segunda nuestra salida tendrá las mismas dimensiones que nuestra entrada.\n",
        "* **Activation:** Función de activación, juega el papel que ya hemos visto en las anteriores clase.\n",
        "* **use_bias:** Me permite definir si mi capa tendrá un vector de preferencia o sesgo (bias vector).\n",
        "\n",
        "* **initializers:** Forma de inicializar mis parámetros, ya sea del kernel o del vector de preferencias.\n",
        "\n",
        "Para este tipo de capa tenemos unos cuantos argumentos más que se pueden consultar en el siguiente [link](https://keras.io/layers/convolutional/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfSokznoYCcv"
      },
      "source": [
        "Veamos como se da la implementación de una capa convolucional en una red, para esto haremos un ejemplo en el cual usando el **fashion mnist** dataset compararemos las diferencias entre la implementación con una red densa multicapa y una red convoluvional. Para eso lo primero que haremos es impotar las librerías necesarias y cargar el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6pl2sdJH14m"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import pandas as pd\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLqD7m5sIGU4"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md796j93JTxe"
      },
      "source": [
        "X_train_full.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKhwkZC2ZE3U"
      },
      "source": [
        "Como vimos en clases pasadas, un punto importante es la normalización de los datos, en este caso nuestros pixeles, esto es lo que realizaremos en el siguiente paso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBgGg2K8JZTM"
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x36V7KqZUV9"
      },
      "source": [
        "En este dataset, recordemos que las etiquetas estan dadas por enteros del 0-9, así que para darle un nombre como tal a cada una debemos tener el siguiente arreglo con las clases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ZrHQLTJbGn"
      },
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5GkHWdKJ-AX"
      },
      "source": [
        "np.unique(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMh2jqdsKBF_"
      },
      "source": [
        "class_names[y_train[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NOBwh5tKEZr"
      },
      "source": [
        "print('X_valid:',X_valid.shape,'\\t','X_test:',X_test.shape,'\\t','X_train:',X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHynirLwaCG8"
      },
      "source": [
        "Veamos algunos de los elementos que podemos encontrar en nuestro dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CePwEg65KLi0"
      },
      "source": [
        "fig , ax =  plt.subplots(3,10, figsize=(15,5))\n",
        "for i , ax in enumerate(ax.flat):\n",
        "  ax.imshow(X_train[i], cmap='binary')\n",
        "  ax.set_axis_off()\n",
        "  ax.set_title(class_names[y_train[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nosa1SsaaKtv"
      },
      "source": [
        "Veamos como sería la implementación de una red densa profunda, la cual ya hemos visto en clases pasadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kexsdMkSKN3L"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAHi3-T3KRWg"
      },
      "source": [
        "model = keras.models.Sequential();\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]));\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"));\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"));\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJjEJot4KWMh"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v47YrY7qKZVu"
      },
      "source": [
        "history =  model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM1m0vksbLOx"
      },
      "source": [
        "Veamos el comportamiento en la curva de validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUYlTFUeK84_"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(10,10))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32RJ3-GiL0Qe"
      },
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHqhWZ4CbSVN"
      },
      "source": [
        "Ahora veamos una implementación de una red neuronal en la cual tenemos dos capas convolucionales en las cuales la Detector stage usaremos como activación la **relu**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgLH9VqlMUtu"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFuCgp2qMwLM"
      },
      "source": [
        "model2= keras.models.Sequential();\n",
        "model2.add(keras.layers.Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)));\n",
        "model2.add(keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\"));\n",
        "model2.add(keras.layers.Flatten());\n",
        "model2.add(keras.layers.Dense(100, activation=\"relu\"));\n",
        "model2.add(keras.layers.Dense(10, activation=\"softmax\"));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odgY9mRYffbq"
      },
      "source": [
        "**Opcional:** Podemos usar el Early stopping, para que nuestro modelo se detenga cuando empiece a aumentar el valor de la función de perdida en el conjunto de validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF9aI8Y2f1fr"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "es=EarlyStopping(monitor='val_loss',patience=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwaGHshmcRNA"
      },
      "source": [
        "Usamos el mismo compilador anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdAjxuuuOTOt"
      },
      "source": [
        "model2.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV_p5UpacZZy"
      },
      "source": [
        "En este caso como usaremos imagenes en escala de grises, debemos hacer un reshape a nuestros datos, de forma tal que los entreguemos en la forma que nuestra red los espera, donde la primer componente hace referencia al número de imagenes, las dos siguientes a las formas de estas (28$\\times$28), y finalamente la última hace referencia a los canales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4gyla5JTmdd"
      },
      "source": [
        "X_train1=X_train.reshape(55000,28,28,1)\n",
        "X_valid1=X_valid.reshape(5000,28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2jpqX3EOfV9"
      },
      "source": [
        "history2=  model2.fit(X_train1, y_train, epochs=20, validation_data=(X_valid1, y_valid),callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cOGt-DsO08k"
      },
      "source": [
        "pd.DataFrame(history2.history).plot(figsize=(10,10))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1T-Wc6nhE09"
      },
      "source": [
        "Podemos ver como una red con una capa convolucional mejora bastante mi modelo, en un número de épocas menor.\n",
        "\n",
        "### **Pooling en keras**\n",
        "Keras además nos permite tener capas de pooling en nuestras redes convolucionales, las cuales se pueden definir de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZuv6xWxuG5A"
      },
      "source": [
        "mod=keras.models.Sequential([\n",
        "      keras.layers.Conv2D(filters=64,kernel_size=3,strides=(1,1),padding='valid',activation='relu',use_bias=True,kernel_initializer='he_uniform', bias_initializer='zeros'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2,2),strides=None,padding='valid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSs_R3w3uoOH"
      },
      "source": [
        "Donde vemos tenemos la capa que explicamos al inicio de esta sección (convolucional) y le añadimos una capa de pooling, donde los argumentos son el tamaño del pool, el stride y el padding. Para más información sobre capas Pooling ir al siguiente [link](https://keras.io/layers/pooling/). Veamos que pasará en el mnist dataset en una red en la cual añadimos una capa de pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrwS2ev_uiNL"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "model3= keras.models.Sequential();\n",
        "model3.add(keras.layers.Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)));\n",
        "model3.add(keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\"));\n",
        "model3.add(keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)));\n",
        "model3.add(keras.layers.Flatten());\n",
        "model3.add(keras.layers.Dense(100, activation=\"relu\"));\n",
        "model3.add(keras.layers.Dense(10, activation=\"softmax\"));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Yqnm63vsgs"
      },
      "source": [
        "model3.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlIehT2Pvxlw"
      },
      "source": [
        "history3=  model3.fit(X_train1, y_train, epochs=20, validation_data=(X_valid1, y_valid),callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUqrrCO8v2Et"
      },
      "source": [
        "pd.DataFrame(history3.history).plot(figsize=(10,10))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_94wIf35xB0W"
      },
      "source": [
        "Donde podemos ver que la diferencia al añadirle esta capa no es mucha para este caso. Sin embargo estas capas son de gran ayuda en ciertos casos."
      ]
    }
  ]
}